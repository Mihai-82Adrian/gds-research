[
  {
    "objectID": "GDS_Concept/lexicon_rebuild_roadmap.html#a.-data-integration-fixes",
    "href": "GDS_Concept/lexicon_rebuild_roadmap.html#a.-data-integration-fixes",
    "title": "Lexicon Rebuild Roadmap (Q4 2025)",
    "section": "A. Data Integration Fixes",
    "text": "A. Data Integration Fixes\n\nCanonical consolidation\n\ncapture OEWM canonical_id and alias bundles during ConceptNet aggregation;\npropagate canonical IDs into the final particle schema;\nmerge alias lists per language; expose preferred lemma + alternates.\n\nBabelNet enrichment keys\n\nattempt lookups by ConceptNet URI → OEWM canonical ID → normalized lemma;\nrecord enrichment provenance per relation (origin = conceptnet | babelnet).\n\nFrequency priors\n\ngeneralise OEWM frequency loader for EN/RO tables (accept optional files);\nattach per-language prior to each particle (persist even when absent)."
  },
  {
    "objectID": "GDS_Concept/lexicon_rebuild_roadmap.html#b.-output-schema-persistence",
    "href": "GDS_Concept/lexicon_rebuild_roadmap.html#b.-output-schema-persistence",
    "title": "Lexicon Rebuild Roadmap (Q4 2025)",
    "section": "B. Output Schema & Persistence",
    "text": "B. Output Schema & Persistence\n\nParticle payload\n\nextend Parquet export with provenance columns (canonical_id, aliases JSON, frequency_prior, embedding_source, affective_source, babelnet_id, source_flags, relations_json);\nkeep JSON columns human-parsable until a nested Arrow schema is justified.\n\nRelation archive\n\ninclude relation metadata in particle rows (JSON) while preparing a future dedicated edges.parquet.\n\nTelemetry snapshot\n\npersist alias-assisted counts, enrichment coverage, and schema version string in telemetry JSON; version stamp the new format."
  },
  {
    "objectID": "GDS_Concept/lexicon_rebuild_roadmap.html#c.-runtime-tooling",
    "href": "GDS_Concept/lexicon_rebuild_roadmap.html#c.-runtime-tooling",
    "title": "Lexicon Rebuild Roadmap (Q4 2025)",
    "section": "C. Runtime & Tooling",
    "text": "C. Runtime & Tooling\n\nCompute pipeline updates\n\nexpose embedding/affective source enums for downstream reasoning;\nretain vector_hash placeholder for future integrity checks.\n\nTesting hooks\n\nregenerate simulation fixtures using the new schema;\nadd CLI flag to export a small “gold” subset for deterministic tests."
  },
  {
    "objectID": "GDS_Concept/lexicon_rebuild_roadmap.html#d.-deliverables-for-this-sprint",
    "href": "GDS_Concept/lexicon_rebuild_roadmap.html#d.-deliverables-for-this-sprint",
    "title": "Lexicon Rebuild Roadmap (Q4 2025)",
    "section": "D. Deliverables for This Sprint",
    "text": "D. Deliverables for This Sprint\n\nCode updates implementing sections A & B (minimum viable schema uplift).\nUpdated documentation (CLAUDE.md, Model Card data section) referencing the enriched schema.\nSynthetic gold_dataset_sample.jsonl (docs/GDS_Deep_Dive/tests/simulation/gold_dataset_sample.jsonl) for simulation replay.\n\n\nNotes: Full canonical merging across languages is staged; this iteration focuses on capturing IDs & metadata so the next pass can collapse duplicates safely."
  },
  {
    "objectID": "GDS_Concept/citations.html",
    "href": "GDS_Concept/citations.html",
    "title": "Source Citations & Attribution",
    "section": "",
    "text": "This project stands on the shoulders of giants. I wish to express my profound gratitude and admiration for the pioneering researchers whose work provided the foundational pillars for this endeavor. The GDS architecture is a synthesis of their brilliant ideas, and this page serves to formally acknowledge their contributions.\nThis annex consolidates the formal citations, acknowledgements, and licensing notes for every external resource that powers the GDS lexicon builder and downstream runtime. Please reference these works in any public release, publication, or demo derived from this project."
  },
  {
    "objectID": "GDS_Concept/citations.html#foundational-concepts-algorithms",
    "href": "GDS_Concept/citations.html#foundational-concepts-algorithms",
    "title": "Source Citations & Attribution",
    "section": "Foundational Concepts & Algorithms",
    "text": "Foundational Concepts & Algorithms\nThis section acknowledges the core scientific and algorithmic concepts that inspired and enabled the GDS architecture.\n\nHyperdimensional Computing (HDC)\n\nCitation: Kanerva, P. (2009). Hyperdimensional computing: An introduction to computing in distributed representation with high-dimensional random vectors. Cognitive Computation, 1(2), 139-159.\nNotes: This work provides the mathematical and conceptual foundation for using high-dimensional binary vectors, which is central to GDS’s SemanticParticle representation and its compositional operators.\n\n\n\nThe Free Energy Principle\n\nCitation: Friston, K. (2010). The free-energy principle: a unified brain theory? Nature Reviews Neuroscience, 11(2), 127-138.\nNotes: Friston’s principle provides the neuroscientific inspiration for GDS’s learning paradigm, where the system acts to minimize surprise by modifying the geometry of its conceptual space.\n\n\n\nGeneral Relativity as a Semantic Analogy\n\nHonorary Mention: Einstein, A. (1916). Die Grundlage der allgemeinen Relativitätstheorie (The Foundation of the General Theory of Relativity). Annalen der Physik, 354(7), 769-822.\nNotes: While not a direct implementation, Einstein’s theory provides the core conceptual metaphor for GDS: concepts as mass warping a semantic spacetime, and reasoning as movement along geodesics.\n\n\n\nK-Shortest Path Algorithm\n\nCitation: Yen, J. Y. (1971). Finding the K Shortest Loopless Paths in a Network. Management Science, 17(11), 712-716.\nNotes: Yen’s algorithm is implemented in the Reasoner to find not just the optimal path, but also the runner-up (“second-best”) path, which is critical for the margin-based contrastive learning loop."
  },
  {
    "objectID": "GDS_Concept/citations.html#knowledge-graphs-embeddings",
    "href": "GDS_Concept/citations.html#knowledge-graphs-embeddings",
    "title": "Source Citations & Attribution",
    "section": "Knowledge Graphs & Embeddings",
    "text": "Knowledge Graphs & Embeddings\n\nBabelNet\n\nCitation: R. Navigli, M. Bevilacqua, S. Conia, D. Montagnini, F. Cecconi. Ten Years of BabelNet: A Survey. Proceedings of IJCAI 2021, pp. 4559–4567.\nNotes: Required for all usages of the daily enrichment pipeline. Access granted under a non-commercial research agreement; respect BabelNet’s license.\n\n\n\nConceptNet 5.x\n\nCitation: Robyn Speer, Joshua Chin, Catherine Havasi. ConceptNet 5.5: An Open Multilingual Graph of General Knowledge. AAAI 2017, pp. 4444–4451. http://aaai.org/ocs/index.php/AAAI/AAAI17/paper/view/14972\nNotes: Dataset distributed under CC BY-SA 4.0. The same reference covers both the graph (assertions.csv) and the downstream integration in the Reasoner.\n\n\n\nConceptNet Numberbatch\n\nCitation (recommended by maintainers): Robyn Speer, Joshua Chin, Catherine Havasi. ConceptNet 5.5: An Open Multilingual Graph of General Knowledge. AAAI 2017, pp. 4444–4451.\nSupplementary attributions (per README): The embeddings ensemble incorporates GloVe, word2vec, OpenSubtitles 2016, and fastText. Cite these works if their contributions are highlighted separately.\nNotes: Embeddings fetched from https://github.com/commonsense/conceptnet-numberbatch (CC BY-SA 4.0)."
  },
  {
    "objectID": "GDS_Concept/citations.html#lexicons-wordnets",
    "href": "GDS_Concept/citations.html#lexicons-wordnets",
    "title": "Source Citations & Attribution",
    "section": "Lexicons & WordNets",
    "text": "Lexicons & WordNets\n\nOpen English WordNet (EN)\n\nCitation: John P. McCrae, Alexandre Rademaker, Francis Bond, Ewa Rudnicka, Christiane Fellbaum. English WordNet 2019 – An Open-Source WordNet for English. Proceedings of the 10th Global Wordnet Conference, 2019.\nNotes: Licensed under CC BY 4.0. Project site: https://en-word.net/.\n\n\n\nOdeNet – Open German WordNet (DE)\n\nCitation: Melanie Siegel, Francis Bond. OdeNet: Compiling a German Wordnet from Other Resources. Proceedings of the 11th Global Wordnet Conference (GWC 2021), pp. 192–198. https://www.aclweb.org/anthology/2021.gwc-1.22\nNotes: Data under CC BY-SA 4.0. Original README distributed with the dump: data/raw/german_wordnet/README.md.\n\n\n\nRomanian WordNet (RoWordNet via RoLLOD)\n\nPrimary citation: Dan Tufiș, Verginica Barbu Mititelu. The Lexical Ontology for Romanian. In Language Production, Cognition, and the Lexicon, Text, Speech and Language Technology, vol. 48, Springer, 2014, pp. 491–504.\nAPI/tooling citation (optional): Ștefan Daniel Dumitrescu, Andrei Marius Avram, Luciana Morogan, Ștefan-Adrian Toma. RoWordNet – A Python API for the Romanian WordNet. ECAI 2018.\nNotes: Distributed under CC BY-SA 4.0; see data/oewm_lexicons/LICENSE.json.\n\n\n\nSEQUOIA Trilingual Lexicon Builder (OEWM Integration)\n\nCitation: Internal toolchain authored by Mihai Adrian Mateescu (Profit Minds). When publishing combined lexicon outputs, attribute the upstream sources listed above and note the CC BY-SA 4.0 redistribution requirement (data/oewm_lexicons/README.md)."
  },
  {
    "objectID": "GDS_Concept/citations.html#affective-emotion-resources",
    "href": "GDS_Concept/citations.html#affective-emotion-resources",
    "title": "Source Citations & Attribution",
    "section": "Affective & Emotion Resources",
    "text": "Affective & Emotion Resources\n\nNRC VAD Lexicon v2\n\nPrimary citation: Saif M. Mohammad. NRC VAD Lexicon v2: Norms for Valence, Arousal, and Dominance for over 55k English Terms. arXiv:2503.23547, 2025.\nFoundational citation (per README): Saif M. Mohammad. Obtaining Reliable Human Ratings of Valence, Arousal, and Dominance for 20,000 English Words. Proceedings of ACL 2018.\nNotes: Point to the homepage http://saifmohammad.com/WebPages/nrc-vad.html in documentation. Usage restricted to research; retain copyright notice.\n\n\n\nGerman Affective Norms (de_emo_norms.txt)\n\nCitation: Maximilian Köper, Sabine Schulte im Walde. Analogies in Complex Verb Meaning Shifts: The Effect of Affect in Semantic Similarity Models. NAACL 2018.\nAdditional acknowledgement (per dataset page): Cite the training resources underpinning the automatically generated norms when relevant (NRC Hashtag Emotion Lexicon, Warriner et al. 2013, Dodds et al. 2011, Brysbaert et al. 2014).\nNotes: Dataset obtained from the IMS Stuttgart resource portal: https://www.ims.uni-stuttgart.de/en/research/resources/experiment-data/de-affect-norms/."
  },
  {
    "objectID": "GDS_Concept/citations.html#operational-requirements",
    "href": "GDS_Concept/citations.html#operational-requirements",
    "title": "Source Citations & Attribution",
    "section": "Operational Requirements",
    "text": "Operational Requirements\n\nMaintain the license headers already present in source and data files.\nInclude these citations in README files, academic papers, slide decks, and any GitHub Pages deployments derived from the Quarto site.\nFor commercial inquiries, contact the respective data providers; most resources listed here are available only for research/non-commercial use (BabelNet, NRC VAD, IMS emotion norms)."
  },
  {
    "objectID": "GDS_Concept/GDS_Concept.html#vision",
    "href": "GDS_Concept/GDS_Concept.html#vision",
    "title": "GDS Concept Overview (English summary)",
    "section": "1. Vision",
    "text": "1. Vision\n\nDevelop a multilingual cognitive engine that models reasoning as geodesic motion through a semantic manifold.\nRepresent each concept as a semantic particle with mass (m₀), charge (q, the hypervector), and affective spin (s, VAD)."
  },
  {
    "objectID": "GDS_Concept/GDS_Concept.html#conceptual-stack",
    "href": "GDS_Concept/GDS_Concept.html#conceptual-stack",
    "title": "GDS Concept Overview (English summary)",
    "section": "2. Conceptual Stack",
    "text": "2. Conceptual Stack\n\nConceptual State Injector (ISC-HDC):\n\nNormalises lemmas (OEWM) and aggregates ConceptNet/BabelNet knowledge.\nProjects Numberbatch embeddings into 20,000-bit binary hypervectors via the Julia backend.\n\nFive-layer architecture: Semantic Base → Proximity Graph → Context Overlay → Geodesic Runtime → Learning Loop.\nLearning: Hebbian updates stored in an overlay with degree normalisation and validation gates."
  },
  {
    "objectID": "GDS_Concept/GDS_Concept.html#why-hdc",
    "href": "GDS_Concept/GDS_Concept.html#why-hdc",
    "title": "GDS Concept Overview (English summary)",
    "section": "3. Why HDC",
    "text": "3. Why HDC\n\nSupports binding, superposition, and role permutations with simple bitwise operations.\nProvides robustness to noise and one-shot learning capabilities.\nEnables compact storage (~2.5 KB per concept) and efficient k-NN search via FAISS binary indices."
  },
  {
    "objectID": "GDS_Concept/GDS_Concept.html#development-roadmap",
    "href": "GDS_Concept/GDS_Concept.html#development-roadmap",
    "title": "GDS Concept Overview (English summary)",
    "section": "4. Development Roadmap",
    "text": "4. Development Roadmap\n\nComplete evaluation suites (cross-lingual retrieval, typed analogy benchmarks).\nDeploy adaptive gating and majority superposition in production.\nPublish technical note and Quarto site with reproducible metrics.\n\nConsult the detailed model card and chronicle docs in docs/GDS_Deep_Dive/ for the full discussion."
  },
  {
    "objectID": "GDS_Concept/GDS_Model_Card_v1.1.html#where-to-find-the-content",
    "href": "GDS_Concept/GDS_Model_Card_v1.1.html#where-to-find-the-content",
    "title": "GDS Model Card (canonical reference)",
    "section": "Where to find the content",
    "text": "Where to find the content\n\n../01_ModelCard_GDS_Details.md – full model card and layered architecture\n../01_ModelCard_P02_ExecutiveSummary.md – executive summary for external readers\n../01_ModelCard_P03_Architecture.md – architecture deep dive\n../01_ModelCard_P04_LearningParadigm.md – learning paradigm details\n../01_ModelCard_P05_Data.md / ../01_ModelCard_P06_Evaluation_Ethics_API.md – data, evaluation, and ethics"
  },
  {
    "objectID": "GDS_Concept/GDS_Model_Card_v1.1.html#how-to-update",
    "href": "GDS_Concept/GDS_Model_Card_v1.1.html#how-to-update",
    "title": "GDS Model Card (canonical reference)",
    "section": "How to update",
    "text": "How to update\n\nEdit the appropriate 01_ModelCard_* file under docs/GDS_Deep_Dive/.\nPreview the documentation (Quarto, Markdown render, etc.) as part of the workflow.\nLeave this shim in place so existing links keep working; it simply points contributors to the maintained source.\n\nCite the model card by linking directly to the canonical files above."
  },
  {
    "objectID": "GDS_Concept/GDS_Project_Chronicle.html#primary-sources",
    "href": "GDS_Concept/GDS_Project_Chronicle.html#primary-sources",
    "title": "GDS Project Chronicle (canonical reference)",
    "section": "Primary sources",
    "text": "Primary sources\n\n../02_Chronicle_GDS.md – concept genesis and interdisciplinary foundations\n../02_Chronicle_P02_Engineering.md – engineering optimisation journey\n../02_Chronicle_P03_Simulation.md – simulation proof of concept\n../02_Chronicle_P04_Evaluation.md – evaluation summary and future directions"
  },
  {
    "objectID": "GDS_Concept/GDS_Project_Chronicle.html#editing-guidance",
    "href": "GDS_Concept/GDS_Project_Chronicle.html#editing-guidance",
    "title": "GDS Project Chronicle (canonical reference)",
    "section": "Editing guidance",
    "text": "Editing guidance\nUpdate the relevant 02_Chronicle_* file and regenerate any exports as needed. Leave this bridge document untouched so downstream tooling continues to resolve references."
  },
  {
    "objectID": "tests/simulation/simulation_report.html",
    "href": "tests/simulation/simulation_report.html",
    "title": "Simulation Report (canonical source)",
    "section": "",
    "text": "Simulation Report (canonical source)\nThe publishable version lives at ../../../../tests/simulation/simulation_report.md in the repository root. This file acts as a bridge for the Quarto site and points readers to the report generated by full_gds_simulation.rs.\nRead it directly here: tests/simulation/simulation_report.md"
  },
  {
    "objectID": "02_Chronicle_GDS.html",
    "href": "02_Chronicle_GDS.html",
    "title": "Part 1: The Genesis of the GDS Concept",
    "section": "",
    "text": "Part 1: The Genesis of the GDS Concept\nThe GDS (Geometrodynamic Semantics) project was not born from an incremental improvement on existing AI paradigms, but from a radical synthesis of ideas from three distinct fields: theoretical physics, neuroscience, and computer science.\n\nThe Core Analogy: Physics of Thought\nThe foundational insight, as detailed in the project’s conceptual documents, is the treatment of semantics through the lens of General Relativity. The core analogy is as follows:\n\nSemantic Space as Spacetime: The model imagines a high-dimensional “conceptual space” that is analogous to the spacetime fabric of the universe.\nInformation as Mass-Energy: Concepts and information are not passive data points. They are treated as possessing “mass-energy”. Just as celestial bodies warp the fabric of spacetime, important concepts (those with high “semantic mass”) curve the conceptual space around them.\nThinking as Geodesic Paths: A “thought” or an act of reasoning is modeled as motion along a geodesic—the path of least resistance through this curved conceptual space. The model doesn’t search through a static database; it follows the natural contours of the dynamically warped semantic landscape. Attention is not a separate mechanism but an emergent property of the geometry itself.\n\n\n\nThe Biological Inspiration: Free Energy Principle\nThis physical metaphor is grounded by a principle from computational neuroscience: Karl Friston’s Free Energy Principle. This principle posits that intelligent systems act to minimize surprise. In the GDS model, this translates to:\n\nLearning as Relaxation: The process of learning is the process of the system “relaxing” into a state of lower surprise. This is achieved by physically altering the metric of the conceptual space. When a reasoning path proves successful, the geometry of the space is changed to make that path “easier” (less surprising) to traverse in the future. This provides a biological justification for the model’s unique learning mechanism.\n\n\n\nThe Practical Implementation: Hyperdimensional Computing (HDC)\nWhile the physics and neuroscience provide the “why”, Hyperdimensional Computing provides the “how”. HDC was chosen as the implementation layer for its unique properties that align perfectly with the GDS vision:\n\nHolographic & Robust: Information in high-dimensional vectors is distributed, making the representations resilient to noise and partial information.\nEfficient Composition: HDC provides simple, fast, and mathematically well-defined operations (bind, bundle) for composing concepts into complex structures, forming the basis of the model’s compositional semantics.\n\nTogether, these three pillars form the foundation of GDS: a model that aims not to compute statistical probabilities, but to simulate the physical dynamics of thought itself.\n\n\n\nPart 2: The Engineering Journey - From 6TB to 7.5GB\nThe ambitious vision of GDS required substantial engineering effort to become practically feasible. The development journey from theoretical concept to operational research prototype involved systematic optimization, documented in the project’s OPTIMIZATION_ROADMAP.md.\n\nThe Initial Crisis: The 5.95 TB Problem\nThe first, naive implementation of the lexicon builder faced an immediate, existential crisis. A direct implementation using standard 64-bit float vectors (f64) for the 20,000-dimensional HDC representations resulted in a projected storage requirement of 5.95 Terabytes for the full lexicon. This was computationally and financially infeasible, blocking the project from the outset.\nThis crisis forced a radical re-evaluation of the storage and data representation strategy, kicking off a multi-stage optimization journey.\n\n\nStage 1: The Binary HDC Breakthrough\nThe first major breakthrough came from aligning the implementation with the theoretical foundations of Hyperdimensional Computing. Canonical HDC research (by Kanerva, Plate, etc.) uses binary or bipolar vectors, not floating-point vectors.\n\nThe Change: The pipeline was refactored to convert the 300D Numberbatch embeddings into 20,000-bit binary vectors. These bits were then packed densely into u64 words.\nThe Impact: This single change had a massive impact. The storage projection plummeted from 5.95 TB to a manageable 121 GB. As a side effect, because binary HDC operations (like Hamming distance) are computationally cheaper than floating-point operations, the entire build process became over 50% faster.\n\n\n\nStage 2: Professionalizing Storage with Parquet\nWhile 121 GB was feasible, it was not yet efficient. The second wave of optimization focused on moving from a simple binary/JSONL file format to a professional, industry-standard solution: Apache Parquet.\n\nThe Change: The ParticleWriter was completely rewritten to use the arrow-rs library, building a proper columnar schema. This introduced several key benefits:\n\nColumnar Storage: Inherently more efficient for analytics and queries.\nZSTD Compression: A modern, high-performance compression algorithm was applied to all columns.\nNative Dictionary Encoding: For string columns with high cardinality (like lemma and concept_id), the Parquet format automatically applied dictionary encoding, further reducing file size.\n\nThe Impact: This professionalization of the storage layer yielded another ~3.5x compression ratio. The final projected size for a high-quality, 3-million-particle lexicon dropped to just 7.5 GB.\n\n\n\nThe Final Result\nThrough this systematic, two-stage optimization process, an ~800x reduction in storage requirements was achieved, transforming an infeasible design into an operational research prototype. This development phase demonstrated that for exploratory research at this scale, aggressive optimization is essential rather than optional.\n\n\n\nPart 3: The Simulation - A Proof of Concept in Action\nTo validate the core principles of the GDS learning paradigm, a controlled experiment was designed and executed. The goal was not merely to test if the code ran, but to determine if the model could exhibit autonomous, non-trivial learning behavior. The entire simulation, from world generation to final analysis, was orchestrated by the full_gds_simulation.rs example.\n\nThe Experimental Setup\n\nA Synthetic World: A small, synthetic world was defined in tests/simulation/world_schema.json. This world contained a few semantic domains (“Royalty”, “Food”), concepts within them (“king”, “queen”, “crown”, “power”), and a set of explicit structural relationships (e.g., king -&gt; HasA -&gt; crown, crown -&gt; SymbolOf -&gt; power).\nA Controlled Lexicon: A dedicated script generated a lexicon from this schema. Crucially, it created clustered HDC vectors, where concepts in the same domain were programmatically made to be “closer” in the high-dimensional space.\nThe Task: The simulation was given a problem: find a path from king to power. This setup created two primary paths:\n\nPath A (Obvious): A direct, one-step path king -&gt; power.\nPath B (Nuanced): A two-step path king -&gt; crown -&gt; power.\n\n\n\n\nThe Four-Step Experiment\nThe simulation then proceeded through a four-step process designed to mirror a cognitive loop of thought, evaluation, and learning.\nStep 1: Initial Reasoning\nThe Reasoner was asked to find the cheapest path. As expected, it chose the most direct and obvious route: king -&gt; power. This established our baseline behavior.\nStep 2: Internal Evaluation\nNext, the simulation applied an internal heuristic—the “coherence score,” calculated as the sum of the semantic mass (m0) of the nodes in a path. This score is designed to favor paths that are semantically richer. The nuanced path, king -&gt; crown -&gt; power, included an extra high-mass concept (“crown”) and thus achieved a higher coherence score. The system autonomously concluded that this path was “better,” even though it was not the cheapest.\nStep 3: Autonomous Learning\nThis internal conclusion triggered a learning event. The learn_edges function was called to modify the Context Overlay. It applied a strong reinforcement (negative cost delta) to the edges of the better path (king -&gt; crown, crown -&gt; power) and a penalty (positive cost delta) to the edge of the losing path (king -&gt; power).\nStep 4: Verification & The Counter-Intuitive Result\nThe Reasoner was run again. The initial expectation was that the path would simply flip to the newly reinforced one. However, the simulation revealed a more complex and interesting behavior: the model still chose the king -&gt; power path.\nAnalysis showed that the cost of this path had decreased, despite being penalized. This counter-intuitive result was traced to the degree normalization in the Reasoner’s cost function. The learning deltas are not applied raw; they are scaled by the connectivity of the nodes. The simulation demonstrated that the model’s behavior is a non-linear, emergent property of its complete architecture, not just a simple sum of its parts. After disabling a secondary “heat diffusion” algorithm to isolate the effect, the learning worked as predicted, and the model changed its preference to the king -&gt; crown -&gt; power path.\n\n\nConclusion of the Experiment\nThis simulation was a profound success. It not only validated the core GDS loop of Reason -&gt; Evaluate -&gt; Self-Reinforce -&gt; Reason Differently, but it also uncovered the non-linear and emergent dynamics of the system. It proved that the GDS model is not a simple toy system but a complex engine whose behavior provides a rich area for academic study.\n\n\n\nPart 4: Final Evaluation & Future Directions\nHaving explored the GDS project from its conceptual foundations to its detailed implementation and simulated behavior, we can draw a comprehensive evaluation of its strengths, weaknesses, and profound potential.\n\nOverall Assessment\nThe GDS prototype demonstrates feasibility of a physics-inspired approach to semantic reasoning. The implementation successfully translates abstract theoretical concepts into functional software, with experimental simulations confirming that core principles produce observable, emergent behaviors in semantic graph navigation.\n\n\nStrengths\n\nExplainability by Design: This is GDS’s most significant advantage over mainstream LLMs. The PathExplain mechanism makes every reasoning process fully transparent and auditable. The ability to see the chosen path, its cost, and the contribution of each component (mass, VAD, learning overlay) is a powerful feature for both debugging and for building trust in the system’s outputs.\nNovel Learning Paradigm: The autonomous learning loop, based on internal evaluation and overlay modification, is a compelling alternative to backpropagation. It is computationally lightweight and more closely mimics theories of neuroplasticity. Our simulation proved that this mechanism works and can change the model’s preferential paths based on experience.\nCompositional Semantics: The move to a “Typed HDC” system with semantic roles (Subject, Object) allows for a level of compositional precision that is difficult to achieve with purely statistical models. It provides a robust framework for understanding and constructing complex ideas from simpler concepts.\nEfficiency and Scalability: The engineering journey from a 6 TB projection to a 7.5 GB reality demonstrates that the underlying architecture is sound and scalable. The use of binary vectors, columnar storage, and efficient indexing makes it possible to handle massive lexicons on commodity hardware.\n\n\n\nWeaknesses & Challenges\n\nComplexity and Tuning: The simulation revealed that the model’s behavior is a non-linear interaction of its many components (cost weights, learning rates, diffusion, degree normalization). This makes the system incredibly powerful but also difficult to tune. Finding the right balance of parameters to achieve desired outcomes will be a significant challenge.\nDependence on Data Quality: Like any knowledge-based system, GDS is highly dependent on the quality and richness of its source data. Biases or gaps in ConceptNet, Numberbatch, or the affective lexicons will be directly inherited by the model’s Semantic Base.\nHeuristic Components: Several key components, such as the “coherence score” used in our simulation or the Supertagger, are currently based on simple heuristics. Developing more robust, data-driven methods for these internal evaluation functions will be critical for advanced applications.\n\n\n\nPotential for an Academic Paper\nThe GDS concept is exceptionally well-suited for academic publication. The argument would be compelling:\n\nContribution: It presents a novel, fully-articulated cognitive architecture that stands in stark contrast to mainstream transformer-based models.\nTheoretical Foundation: The work is deeply grounded in established theories from physics (General Relativity), neuroscience (Free Energy Principle), and computer science (HDC), providing a rich theoretical backdrop.\nEmpirical Evidence: The simulation provides a concrete, reproducible experiment demonstrating the core principles of autonomous learning and emergent behavior. The counter-intuitive result from the simulation is not a failure, but the core of a fascinating discussion about non-linear dynamics in complex systems.\nKey Differentiator: The emphasis on explainability is a direct answer to one of the biggest criticisms leveled against modern AI, making the work highly relevant.\n\nA paper could be structured around the journey we took: outlining the theory, describing the architecture that implements it, and presenting the simulation as an empirical validation of the theory’s potential, including an analysis of its complex, emergent behaviors.\nIn conclusion, the GDS project is not just a functional lexicon builder; it is a successful, large-scale experiment that validates a powerful and original vision for artificial intelligence."
  },
  {
    "objectID": "01_ModelCard_P02_ExecutiveSummary.html",
    "href": "01_ModelCard_P02_ExecutiveSummary.html",
    "title": "Part 2: Executive Summary & Vision",
    "section": "",
    "text": "Part 2: Executive Summary & Vision\n\n\n\n\n\n\nAbout This Research\n\n\n\nThis document presents Geometrodynamic Semantics (GDS), an independent research exploration into physics-inspired artificial intelligence. This work is conducted as part of the Independent Research & Development Genesis initiative by Mihai A. Mateescu. For background and collaboration opportunities, see About the Researcher.\n\n\nGDS (Geometrodynamic Semantics) is a research prototype exploring an alternative to traditional, statistics-based Transformer architectures. Rather than predicting the next token, GDS models semantic reasoning as a physical phenomenon.\nInspired by Einstein’s theory of General Relativity, GDS treats concepts as “semantic particles” possessing intrinsic properties: mass (semantic importance), charge (the hyperdimensional vector), and spin (affective value). These particles are generated by the CSI-HDC (Conceptual State Injector using Hyperdimensional Computing)—a semantic tokenizer that replaces traditional token sequences with 20,000-dimensional binary hypervectors.\nThe CSI-HDC’s output is not a flat sequence of tokens, but a dynamic field of interacting particles. When processed by the GDS engine, this field warps a high-dimensional “conceptual space”. Reasoning is then modeled as finding the path of least resistance—a geodesic—through this curved semantic manifold.\nLearning occurs not through backpropagation, but through a Hebbian-style mechanism that modifies the geometry of the space itself. A dynamic Overlay layer adds contextual adjustments to edge costs in the graph. Successful reasoning paths are reinforced, making them “cheaper” and more likely in future queries. This process is governed by internal evaluation and a ValidationGate, enabling autonomous learning based on coherence principles rather than direct supervision.\nThe result is a research prototype demonstrating efficient, scalable, and—most importantly—explainable semantic reasoning, where every path can be audited and understood step-by-step."
  },
  {
    "objectID": "00_About_Research.html#about-the-research",
    "href": "00_About_Research.html#about-the-research",
    "title": "About the Researcher",
    "section": "About the Research",
    "text": "About the Research\nThis research blog documents the Independent Research & Development Genesis initiative, exploring novel paradigms in artificial intelligence and machine learning through a physics-inspired lens.\n\nBackground & Approach\nMy academic background is in Finance and Accounting, not computer science. This project represents a self-funded research effort driven by curiosity about fundamental questions: Can we model cognition using physics and neuroscience principles instead of pure statistics? Can hyperdimensional computing provide a more interpretable foundation for semantic reasoning?\nThe ideas, research direction, and architecture emerged from my exploration of cognitive science, physics, and neuroscience literature. The implementation leverages modern AI-assisted development tools (Claude, Codex, Gemini) to translate conceptual designs into functional code—a pragmatic approach that allows domain exploration without deep programming expertise.\n\n\nResearch Goals\nThis work aims to:\n\nValidate a novel paradigm: Physics-inspired semantic reasoning with HDC vectors\nDocument the research journey: From concept to working prototype\nEngage the academic community: Seeking feedback, collaboration, and mentorship\nExplore formalization opportunities: Open to PhD supervision or research partnerships in AI/ML/Cognitive Science\n\n\n\nCurrent Status\nGDS is a proof-of-concept research prototype demonstrating:\n\nFunctional HDC-based semantic tokenization (20,000-dimensional vectors)\n~800x storage optimization through binary representations\nAutonomous learning via physics-inspired validation gates\nMultilingual semantic reasoning capabilities\n\nThis is exploratory research, not a commercial product. All findings, code, and methodology are documented transparently to enable reproducibility and scholarly discussion.\n\n\nContact & Collaboration\nI welcome dialogue with:\n\nAcademic researchers in AI, ML, neuroscience, or cognitive science\nPotential PhD supervisors interested in novel AI paradigms\nCollaborators exploring alternative approaches to language understanding\n\nContact: mihai.mateescu@web.de\n\nNote: This research is conducted independently without institutional affiliation or commercial backing. It represents a personal exploration of ideas at the intersection of physics, neuroscience, and artificial intelligence."
  },
  {
    "objectID": "01_ModelCard_P05_Data.html",
    "href": "01_ModelCard_P05_Data.html",
    "title": "Part 5: Data & Lexicon Construction (CSI-HDC)",
    "section": "",
    "text": "Part 5: Data & Lexicon Construction (CSI-HDC)\nThe foundation of the GDS system is the Semantic Base, a large-scale lexicon of “semantic particles” generated by the CSI-HDC (Conceptual State Injector using Hyperdimensional Computing) pipeline. This pipeline processes and synthesizes information from multiple data sources to generate concept representations with physics-inspired properties.\n\nPrimary Data Sources\n\n\n\n\n\n\n\n\nData Source\nLocation in Project\nRole & Contribution\n\n\n\n\nConceptNet\ndata/raw/assertions.csv\nProvides the primary structural graph of common-sense relationships (e.g., UsedFor, CapableOf, PartOf). It forms the backbone of explicit knowledge.\n\n\nNumberbatch\ndata/raw/numberbatch.txt\nPre-trained 300-dimensional word embeddings. Primary source for generating 20,000-bit binary HDC vectors via the Julia HDC server, and fallback for affective score calculation.\n\n\nNRC-VAD Lexicon\ndata/raw/nrc_vad/\nProvides affective scores for English words across three dimensions: Valence (pleasure/displeasure), Arousal (intensity), and Dominance (control). This is the source for the spin property of English particles.\n\n\nGerman Norms\ndata/raw/german_norms/\nThe German equivalent of the NRC-VAD lexicon, providing affective scores for German words.\n\n\nOEWM Lexicons\ndata/oewm_lexicons/\nOpen English, German, and Romanian WordNet data. This is a crucial source for normalization, synonymy (aliases), and word frequency priors. It significantly boosts the quality of mass calculation and the coverage of other lookups.\n\n\nBabelNet Cache\ndata/enrichment/babelnet_cache.db\nLocal SQLite database caching BabelNet API results. Used in enrichment iterations to add high-quality multilingual relations, expanding the semantic graph over time.\n\n\n\n\n\nThe Generation Pipeline (LexiconBuilder)\nThe process is orchestrated by the LexiconBuilder in the Rust codebase and follows several key stages:\n\n\n\nLexiconBuilder Pipeline Flow\n\n\n\nAggregation: Raw assertions from ConceptNet are streamed and aggregated into a per-concept map, building a preliminary list of relations.\nNormalization & Enrichment: Lemmas are normalized using OEWM. This step also discovers aliases (synonyms) that will be used in later stages.\nQuality Scoring: Each potential concept is scored based on a set of heuristics: its connectivity in the graph, whether it has a Numberbatch embedding, and its coverage in affective lexicons.\nFiltering: Concepts that do not meet a minimum quality threshold (e.g., min_relations) are discarded.\nProperty Calculation: For each high-quality concept:\n\nMass (m0) is calculated from graph connectivity, boosted by frequency priors from OEWM.\nSpin (s) is derived from affective lexicons (NRC-VAD, German Norms).\nCharge (q) is generated by the Julia HDC server, which expands 300D Numberbatch embeddings into 20,000-bit binary hypervectors—this is the CSI-HDC tokenization step that replaces traditional token embeddings.\n\nExport: The final collection of SemanticParticle objects is written to a compressed Parquet file (ZSTD compression), forming the Semantic Base for the GDS runtime."
  },
  {
    "objectID": "01_ModelCard_P06_Explainability.html#why-explainability-matters-for-gds",
    "href": "01_ModelCard_P06_Explainability.html#why-explainability-matters-for-gds",
    "title": "Part 6: Explainability & Interpretability",
    "section": "Why Explainability Matters for GDS",
    "text": "Why Explainability Matters for GDS\nTraditional language models face the “black box problem” – their reasoning emerges from billions of learned weights that humans cannot interpret. GDS takes a fundamentally different approach:\n\nGraph-based reasoning provides natural traceability (every path is a sequence of concepts)\nPhysics-inspired cost functions make decisions based on interpretable properties (mass, distance, frequency)\nExplicit uncertainty quantification separates knowledge limits from data noise\nMeta-cognitive chains document the system’s reasoning process step-by-step\n\nThis makes GDS particularly suitable for high-stakes applications (legal, medical, financial) where decisions must be justified and audited."
  },
  {
    "objectID": "01_ModelCard_P06_Explainability.html#meta-cognition-chain",
    "href": "01_ModelCard_P06_Explainability.html#meta-cognition-chain",
    "title": "Part 6: Explainability & Interpretability",
    "section": "1. Meta-Cognition Chain",
    "text": "1. Meta-Cognition Chain\nThe Meta-Cognition Chain provides a transparent record of GDS’s reasoning process, decomposed into four steps that mirror human problem-solving:\n\nChain Structure\n1️⃣ Analysis      → Understand the query and assess available knowledge\n2️⃣ Tools         → Execute graph search in the knowledge base\n3️⃣ Synthesis     → Evaluate path quality and formulate answer\n4️⃣ Response      → Deliver final answer with confidence level\n\n\nConfidence Levels\nGDS explicitly quantifies confidence using three levels:\n\nHigh: Based on verified information with optimal path quality (low cost &lt; 5.0, high margin &gt; 1.0)\nLow: Uncertain due to high path cost (&gt; 10.0) or low margin (&lt; 0.5)\nRejection: Insufficient data or expertise to answer reliably\n\n\n\nExample Chain\nFor the query \"What connects 'go_to_barber' to 'supermarket'?\":\n1️⃣ Analysis\n“Analyzing semantic relationship between start concept ‘go_to_barber’ (m₀=11.108) and target concept ‘supermarket’ (m₀=9.534) in the knowledge graph.”\nConfidence: High (both concepts have strong frequency indicators)\n2️⃣ Tools (Knowledge Graph Search)\n“Found path with 4 nodes, total cost 2.844. Nodes: go_to_barber → cut_hair → mass → supermarket”\nResult: Path successfully discovered\n3️⃣ Synthesis\n“Based on the path found in the knowledge graph, with optimal cost 2.844 and safety margin 0.797, I formulate a verified answer.”\nPath Quality: Low cost indicates strong semantic connection\n4️⃣ Response\n“‘go_to_barber’ is connected to ‘supermarket’ through 2 intermediate concepts: ‘cut_hair’, ‘mass’. The total path cost is 2.844, which indicates a strong semantic connection.”\nConfidence: High\nConfidence Explanation:\n“Confidence based on: low optimal cost, short path (direct connection), high-frequency nodes (m₀=7.98).”"
  },
  {
    "objectID": "01_ModelCard_P06_Explainability.html#attribution-analysis",
    "href": "01_ModelCard_P06_Explainability.html#attribution-analysis",
    "title": "Part 6: Explainability & Interpretability",
    "section": "2. Attribution Analysis",
    "text": "2. Attribution Analysis\nAttribution identifies which concepts and relationships contribute most to a reasoning outcome. GDS uses frequency-weighted positional attribution to score node importance:\n\nAttribution Formula\n\\[\n\\text{Importance}(node) = \\frac{m_0(node) \\times \\text{position\\_weight}}{\\sum_{n \\in path} m_0(n)}\n\\]\nWhere: - m₀ (mass) = concept frequency/importance from lexicon (0-20 scale) - position_weight = higher for concepts in central path positions (avoids bias toward start/goal)\n\n\nCritical Nodes\nNodes with importance &gt; 80% are marked as critical – removing or weakening these nodes would significantly alter the reasoning outcome.\n\n\nVisualization Example\nIn the saliency map visualization: - 🔴 Red nodes (High importance: 0.8-1.0): Core concepts that anchor the semantic path - 🟠 Orange nodes (Medium-High: 0.6-0.8): Supporting concepts that strengthen the connection - 🟡 Yellow nodes (Medium: 0.4-0.6): Contextual concepts that provide nuance - 🟢 Green/Blue nodes (Low: 0.0-0.4): Peripheral concepts with minimal impact\nEdge thickness reflects the contribution weight (thicker = stronger semantic relationship).\n\n\nMetrics Displayed\n\nCritical Nodes: Count of nodes with importance &gt; 0.8\nTotal Nodes: Full path length\nAverage Importance Score: Mean attribution across all nodes\nAttribution Method: Frequency-weighted positional attribution"
  },
  {
    "objectID": "01_ModelCard_P06_Explainability.html#uncertainty-quantification",
    "href": "01_ModelCard_P06_Explainability.html#uncertainty-quantification",
    "title": "Part 6: Explainability & Interpretability",
    "section": "3. Uncertainty Quantification",
    "text": "3. Uncertainty Quantification\nGDS decomposes uncertainty into two independent components following established uncertainty theory:\n\nTypes of Uncertainty\n\n📚 Epistemic Uncertainty (Knowledge Limits)\n\nDefinition: Uncertainty due to incomplete knowledge or missing data in the graph\nCalculation: Variability in path costs across alternative routes (standard deviation)\nInterpretation: High epistemic uncertainty suggests the system needs more data or connections\nVisual Encoding: Blur effect on nodes (more blur = higher epistemic uncertainty)\nThresholds: Low (0-15%), Medium (15-20%), High (&gt;20%)\n\n\n\n🎲 Aleatoric Uncertainty (Data Noise)\n\nDefinition: Inherent randomness in concept relationships (cannot be reduced by adding more data)\nCalculation: Based on concept frequency stability and edge weight variance\nInterpretation: High aleatoric uncertainty indicates ambiguous or context-dependent concepts\nVisual Encoding: Stripe pattern overlay (more stripes = higher aleatoric uncertainty)\nThresholds: Low (0-15%), Medium (15-20%), High (&gt;20%)\n\n\n\n📊 Total Uncertainty\n\nCombination: \\(\\text{Total} = \\sqrt{\\text{Epistemic}^2 + \\text{Aleatoric}^2}\\)\nVisual Encoding: Pulse animation for nodes with total uncertainty &gt; 30%\nAccessibility: Animation respects prefers-reduced-motion system setting\n\n\n\n\nConfidence Interval (95% CI)\nThe system reports a 95% confidence interval for path quality: - Narrow interval (e.g., [40%-45%]): High certainty, reliable answer - Wide interval (e.g., [23%-57%]): Low certainty, answer is tentative\nExample: CI [23% - 57%] with width 34% indicates high uncertainty – alternative paths exist with similar costs, making the choice sensitive to small variations."
  },
  {
    "objectID": "01_ModelCard_P06_Explainability.html#interactive-visualizations",
    "href": "01_ModelCard_P06_Explainability.html#interactive-visualizations",
    "title": "Part 6: Explainability & Interpretability",
    "section": "4. Interactive Visualizations",
    "text": "4. Interactive Visualizations\nGDS provides three HTML-based visualizations optimized for research transparency and public communication:\n\n\n\n\n\n\n🎯 Try the Live Demos\n\n\n\nExplore real XAI visualizations generated from actual reasoning queries:\n\n🗺️ Saliency Map Demo - Interactive node importance visualization\n🔄 Counterfactual Scenarios Demo - What-if analysis explorer\n📊 Complete XAI Dashboard Demo - Unified explainability view\n\nAll demos are fully interactive with export capabilities (PDF/SVG).\n\n\n\n🗺️ Saliency Map (Node & Edge Importance)\nPurpose: Show which concepts and relationships drive the reasoning outcome.\nFeatures: - Color-coded nodes by importance (red = critical, blue = peripheral) - Edge thickness proportional to contribution weight - Interactive tooltips showing: - Concept lemma and importance score - Frequency (m₀) value - Uncertainty breakdown (epistemic, aleatoric, total) - Uncertainty glyphs (blur, stripes, pulse) - Responsive SVG with semantic HTML landmarks\nUse Case: Understand which concepts are most influential in connecting two ideas.\n\n\n\n🔄 Counterfactual Scenarios (What-If Analysis)\nPurpose: Explore how reasoning would change under different conditions.\nScenarios Generated: 1. Remove Expensive Edge (e.g., “Remove edge 127→2, cost 1.04”) - Shows impact of eliminating a weak connection - Displays alternative path cost comparison 2. Use Alternative Path (e.g., “Use runner-up path, +0.80 cost”) - Reveals second-best reasoning route - Quantifies decision margin\nMetrics Displayed: - Impact %: How much the counterfactual affects path cost (36% = major change, 28% = moderate) - Original vs Alternative: Side-by-side comparison of path costs - Confidence Interval: Shows uncertainty range for the current path\nContrastive Explanation:\n“Alternative paths exist with similar costs. The choice is sensitive to small variations.”\n→ This indicates low decision margin – the system’s choice is not strongly preferred.\n\n\n\n📊 Complete XAI Dashboard\nPurpose: Unified view of all explainability features in a single page.\nSections: 1. Meta-Cognition Chain (4-step reasoning process) 2. Saliency Map (embedded SVG with uncertainty glyphs) 3. Attribution Metrics (critical nodes, average score, method) 4. Uncertainty Quantification (epistemic, aleatoric, total, 95% CI) 5. Counterfactual Scenarios (2 alternative scenarios) 6. Contrastive Explanation (why this path?)\nExport Options: - PDF Export (A4 format, 2× quality, metadata preserved) - SVG Download (vector format, preserves filters and patterns)"
  },
  {
    "objectID": "01_ModelCard_P06_Explainability.html#accessibility-standards-compliance",
    "href": "01_ModelCard_P06_Explainability.html#accessibility-standards-compliance",
    "title": "Part 6: Explainability & Interpretability",
    "section": "5. Accessibility & Standards Compliance",
    "text": "5. Accessibility & Standards Compliance\nAll visualizations are designed with accessibility-first principles:\n\nWCAG 2.1 AA Compliance\n✅ 0 violations (validated with axe-core 4.11.0)\nAccessibility Features: - Semantic HTML: Proper &lt;nav&gt;, &lt;main&gt;, &lt;section&gt; landmarks - ARIA Labels: All interactive elements have descriptive labels - Example: aria-label=\"Node: go_to_barber, Importance: 100%, Epistemic Uncertainty: 0%, Aleatoric Uncertainty: 4%\" - Color Contrast: All text meets 4.5:1 minimum ratio (AA standard) - Updated button colors from #667eea (4.0:1) → #5a67d8 (4.57:1) - Keyboard Navigation: Full support for tab navigation and focus indicators - Screen Reader Support: All visualizations include text alternatives - Reduced Motion: Pulse animations respect prefers-reduced-motion system preference\n\n\nResearch Contribution: Uncertainty Glyphs\nGDS introduces novel visual encodings for uncertainty representation:\n\nEpistemic Blur (SVG &lt;filter&gt; with Gaussian blur, 3 levels)\nAleatoric Stripes (SVG &lt;pattern&gt; with diagonal lines, 3 densities)\nHigh Uncertainty Pulse (CSS @keyframes animation with accessibility override)\n\nThese glyphs allow dual-channel encoding: color represents importance, while visual effects represent uncertainty – enabling richer information density without sacrificing clarity."
  },
  {
    "objectID": "01_ModelCard_P06_Explainability.html#technical-implementation",
    "href": "01_ModelCard_P06_Explainability.html#technical-implementation",
    "title": "Part 6: Explainability & Interpretability",
    "section": "6. Technical Implementation",
    "text": "6. Technical Implementation\n\nCore Technologies\n\nLanguage: Rust (for performance and memory safety)\nGraph Representation: Custom SVG generation with physics-inspired layout\nExport Libraries:\n\njsPDF 2.5.2 (PDF generation)\nhtml2canvas 1.4.1 (HTML-to-canvas rendering)\n\nValidation: axe-core 4.11.0 (accessibility testing)\n\n\n\nFile Structure\nsrc/reasoner/\n├── visualization.rs       # HTML/SVG generators (1,315 lines)\n│   ├── SaliencyMapGenerator\n│   ├── CounterfactualUIGenerator\n│   └── XAIDashboard\n├── xai.rs                 # Attribution & counterfactual logic (605 lines)\n├── metacognition.rs       # Confidence chain builder (354 lines)\n└── mod.rs                 # Main reasoner with XAI integration (599 lines)\n\n\nExample Usage\nuse gds_core::reasoner::{Reasoner, SaliencyMapGenerator};\n\nlet reasoner = Reasoner::new(&graph, &overlay, params);\nlet (path, metacog, xai) = reasoner.reason_with_xai(start, goal, &constraints)?;\n\n// Generate saliency map visualization\nlet generator = SaliencyMapGenerator::new();\nlet html = generator.generate_html(\n    &path,\n    &xai.attribution,\n    &xai.uncertainty,\n    \"Query: concept_A → concept_B\",\n);\n\nstd::fs::write(\"saliency_map.html\", html)?;\n\n\nGenerated Output\nThree standalone HTML files (no external dependencies): - saliency_map.html (node importance + uncertainty glyphs) - counterfactuals.html (what-if scenarios + contrastive explanation) - dashboard.html (unified XAI view with all features)"
  },
  {
    "objectID": "01_ModelCard_P06_Explainability.html#summary",
    "href": "01_ModelCard_P06_Explainability.html#summary",
    "title": "Part 6: Explainability & Interpretability",
    "section": "Summary",
    "text": "Summary\nGDS’s explainability framework addresses the black box problem through:\n\nTransparent Reasoning: Graph paths are inherently interpretable\nConfidence Quantification: Explicit high/low/rejection levels with justifications\nUncertainty Decomposition: Separate epistemic (knowledge) from aleatoric (noise) uncertainty\nAttribution Analysis: Identify which concepts drive outcomes\nCounterfactual Exploration: Test “what if” scenarios to validate decisions\nAccessible Visualizations: WCAG 2.1 AA compliant, professional-quality outputs\n\nThis makes GDS suitable for research transparency, regulatory compliance, and public communication – domains where decision justification is not optional but mandatory.\n\nNext: Project Chronicle →\nPrevious: ← Data Sources"
  },
  {
    "objectID": "01_ModelCard_P03_Architecture.html",
    "href": "01_ModelCard_P03_Architecture.html",
    "title": "Part 3: Detailed Architecture (How the System “Reasons”)",
    "section": "",
    "text": "Part 3: Detailed Architecture (How the System “Reasons”)\nThe GDS cognitive architecture is a multi-layered research prototype where each layer has a distinct responsibility, from static data storage to dynamic, adaptive learning. The reasoning process emerges from the interaction of these layers.\n\nThe 5 Layers of GDS\n\n\n\nGDS Cognitive Architecture Layers\n\n\n\nSemantic Base (The Static Universe)\n\nComponent: A large-scale, compressed Parquet file containing the lexicon of “semantic particles” generated by CSI-HDC (Conceptual State Injector using Hyperdimensional Computing).\nRole: This serves as the foundational semantic memory of the system. It stores concepts with pre-calculated properties: mass (m0), affective spin (VAD), and 20,000-bit binary HDC vectors (q) that replace traditional token embeddings.\nIncludes: A static graph of structural edges derived from curated knowledge bases (e.g., ConceptNet’s IsA relation).\n\nProximity Graph (The Implicit Network)\n\nComponent: A graph layer constructed on top of the Semantic Base, featuring proximity edges.\nRole: These edges are not explicit in source data but discovered via k-Nearest Neighbors (k-NN) search (using FAISS) on the HDC vectors. This enables novel connections between semantically similar concepts, even without explicit knowledge base links. This graph represents the fabric of the semantic manifold.\n\nContext Overlay (The Adaptive Layer)\n\nComponent: A dynamic key-value store (LMDB) that maps graph edges to delta adjustments.\nRole: This implements short-term, contextual memory through temporary adjustments to edge traversal costs. During learning, the static graph remains unchanged; the system adds positive (penalty) or negative (reinforcement) deltas to this overlay. It is volatile and session-specific by default.\n\nGeodesic Runtime (The Reasoner)\n\nComponent: The Reasoner module implementing graph-traversal (A* algorithm).\nRole: This is the active reasoning component. Given start and goal concepts, the Reasoner finds the path of least cost rather than simply the shortest path. The cost function implements “geodesic” behavior through a weighted sum:\n\n$Cost(edge) = \\alpha \\cdot (1/m_0) + \\beta \\cdot (\\Delta VAD) + \\gamma \\cdot (1/rel_{strength}) + \\lambda \\cdot (Overlay_{\\Delta})$\n\nThe system naturally prefers paths through important concepts (high m0), avoids sharp emotional shifts (low ΔVAD), follows strong structural relations, and adapts based on learned overlay deltas.\n\nLearning Loop (The Adaptive Mechanism)\n\nComponent: The learn and gating modules.\nRole: This layer implements autonomous adaptation. Following a reasoning task, evaluation can trigger a learning event. The learn_edges function applies Hebbian-style updates to the Context Overlay. A ValidationGate then determines whether temporary changes improve performance on evaluation tasks before consolidation into a versioned overlay snapshot.\n\n\n\n\nSummary of a Reasoning Cycle\nA GDS reasoning cycle proceeds as follows:\n\nA query initiates a search for a low-cost path between two concepts in the Proximity Graph.\nThe Reasoner explores the graph, calculating step costs using the multi-faceted cost function that reads from both the static Semantic Base and the dynamic Context Overlay.\nThe lowest-cost path is returned as the reasoning result.\nBased on evaluation, the Learning Loop may update the Context Overlay, reinforcing or penalizing edges and thus modifying the geometric structure for future queries."
  },
  {
    "objectID": "02_Chronicle_P05_Training_Journey_v2.html",
    "href": "02_Chronicle_P05_Training_Journey_v2.html",
    "title": "Part 5: The Training Journey",
    "section": "",
    "text": "🌌 Epoch 1-2: The Genesis\n    The Birth of Structure from Chaos\n    \n        In the beginning, there was dependency. The GDS reasoner, freshly initialized with synthetic \n        reasoning patterns, leaned heavily on its bootstrap scaffolding. Natural data was scarce, \n        and the model had no choice but to rely on the artificially generated inference pathways that gave it structure.\n    \n    \n        This is the Genesis Phase — where every query echoes through the synthetic corridors of the \n        knowledge graph. The streamgraph visualization captures this dependency beautifully: the synthetic \n        layer dominates, while natural data forms only a thin thread beneath.\n    \n    \n        Synthetic reliance: ~80% of inference came from bootstrap patterns\n        Natural signals: Barely visible in the early epochs\n        Core insight: The model wasn't learning yet—it was repeating\n    \n    \n        This is where the journey begins: a model that depends on its training wheels before \n        it can learn to ride on its own.\n    \n    \n    \n    \n        Data Reliance Stream: Natural vs. Synthetic Inferences\n        \n    \n\n\n\n\n    📊 Epoch 1-3: The Twelve Paths\n    Mapping the Landscape of Query Costs\n    \n        As training progressed, the model faced twelve archetypal queries—each representing a \n        different semantic challenge. Some were simple, some were complex, and some were deliberately ambiguous.\n    \n    \n        The small multiples visualization reveals the divergent trajectories of these queries:\n    \n    \n        Q1-Q4: Low-cost queries with stable convergence\n        Q5-Q8: Mid-range complexity with fluctuating costs\n        Q9-Q12: High-cost outliers that resisted optimization\n    \n    \n        This was the first hint of a fundamental truth: not all queries are created equal. \n        The model was learning, but selectively—favoring certain semantic patterns while \n        struggling with others.\n    \n    \n    \n    \n        Query Cost Trajectories: Small Multiples View\n        \n    \n\n\n\n\n    🎯 Epoch 1-3: The Confidence Paradox\n    When Margins Stabilize But Paths Remain Costly\n    \n        By Epoch 3, something unexpected emerged: the confidence margins stabilized. \n        The model was becoming more certain about its chosen paths—but those paths were still expensive.\n    \n    \n        The confidence ribbon visualization captures this paradox beautifully:\n    \n    \n        Tightening ribbons: Variance decreased across epochs\n        Stable costs: But the mean path cost remained stubbornly high\n        The insight: The model was confidently wrong\n    \n    \n        This was the moment of realization: confidence is not correctness. The model had learned \n        to be consistent, but it hadn't learned to be efficient. It was choosing expensive paths with \n        unwavering certainty.\n    \n    \n    \n    \n        Confidence Margins: Path Cost Distribution Over Time\n        \n    \n\n\n\n\n    🏔️ The Ridgeline Revelation\n    Discovering the Three Performance Clusters\n    \n        The ridgeline distribution revealed a hidden structure: three distinct performance clusters \n        emerged across all epochs:\n    \n    \n        Low-Cost Peak: ~20% of queries converged to near-optimal paths\n        Mid-Range Plateau: ~50% hovered in mediocrity\n        High-Cost Outliers: ~30% remained stubbornly expensive\n    \n    \n        This trimodal distribution was the smoking gun. It revealed that the model wasn't learning uniformly—it was \n        fragmenting the semantic space into easy, medium, and hard \n        categories based on the quality of the underlying data.\n    \n    \n        The ridgeline told the story: data quality trumps algorithmic sophistication. No amount of \n        training could fix queries that lacked semantic scaffolding in the knowledge graph.\n    \n    \n    \n    \n        Performance Distribution: Ridgeline Density Plot\n        \n    \n\n\n\n\n    🔍 The Pathfinder: Q4 Dissected\n    Inside the Inference Graph\n    \n        Query 4 became the case study—a representative example of the model's reasoning process. \n        The network graph visualization exposes the full inference pathway:\n    \n    \n        Start node: The query concept (highlighted in cyan)\n        Intermediate hops: Semantic bridges traversed during reasoning\n        End node: The final inferred concept\n        Edge weights: Costs determined by mass, VAD, and learning overlay\n    \n    \n        What the graph reveals is explainability by design. Unlike black-box neural networks, \n        every step of the GDS reasoning process is auditable. You can see:\n    \n    \n        Which concepts were considered\n        Which edges were traversed\n        Why certain paths were preferred over others\n    \n    \n        This is the promise of geometrodynamic semantics: reasoning that is not just powerful, \n        but transparent.\n    \n    \n    \n    \n        Query 4 Pathfinding: Inference Graph Visualization\n        \n    \n\n\n\n\n    ✨ Epilogue: The Path Forward\n    From Diagnosis to Solution\n    \n        The training journey revealed a fundamental truth about machine learning: data quality trumps \n        algorithmic sophistication.\n    \n    \n        The visualizations told a story of:\n    \n    \n        Synthetic dependence: 97% hallucinated connections\n        Uneven learning: Bimodal difficulty distribution\n        Confident ignorance: Stable margins on unstable foundations\n        Semantic voids: Unbridgeable gaps in the knowledge graph\n    \n    \n        The solution wasn't to train harder or longer. It was to enrich the semantic substrate:\n    \n    \n        BabelNet integration: Expanded coverage from 15% to 80%\n        Multi-lingual bridges: Connected isolated semantic islands\n        Domain enrichment: Filled critical gaps in specialized vocabularies\n    \n    \n        The journey from v2 to v5 wasn't a story of algorithmic triumph. It was a story of \n        diagnostic humility — learning to read the signals hidden in the noise, \n        understanding that sometimes the model is telling you \"I can't learn because you haven't given me what I need to know.\"\n    \n    \n        This is the art of machine learning: listening to what the data cannot say."
  },
  {
    "objectID": "01_ModelCard_P04_LearningParadigm.html",
    "href": "01_ModelCard_P04_LearningParadigm.html",
    "title": "Part 4: Learning Paradigm",
    "section": "",
    "text": "Part 4: Learning Paradigm\nThe GDS learning paradigm is fundamentally different from the backpropagation and gradient descent methods that power traditional Large Language Models. It is a form of autonomous, Hebbian-style learning that modifies the geometry of the conceptual space in response to experience.\n\nCore Principles\n\nNo Backpropagation: The model does not compute gradients across a massive neural network. Learning is a local, lightweight process.\nLearning by Modifying Costs: Instead of adjusting neuron weights, GDS learns by adjusting the “cost” of traversing specific edges in the semantic graph. This is done by writing small delta values to the dynamic Context Overlay.\nReinforcement and Penalization: Paths that lead to successful or “coherent” outcomes are reinforced (their edges receive a negative delta, making them cheaper and more attractive to the Reasoner). Paths that are evaluated as poor alternatives are penalized (their edges receive a positive delta, making them more expensive).\nInternal Evaluation: The model does not strictly require external, supervised labels to learn. As demonstrated in our simulation, it can employ internal heuristics (such as a “coherence score” based on concept mass) to decide which paths are “better” and thus worthy of reinforcement.\nStability and Explainability: Because learning only affects the overlay, the foundational knowledge graph remains stable. The changes are auditable (one can inspect the deltas in the overlay) and their effect is directly observable in the Reasoner’s behavior and cost calculations.\n\n\n\nCase Study: The Simulation\nOur simulation provided a perfect, concrete example of this paradigm in action:\n\nInitial State: The Reasoner initially chose the cheapest, most obvious path: king -&gt; power.\nInternal Evaluation: An internal metric, the “coherence score” (sum of concept masses), evaluated the alternative path king -&gt; crown -&gt; power as being semantically richer, despite its higher initial cost.\nAutonomous Learning: This internal evaluation triggered a learning event. The learn_edges function was called to apply a strong negative delta (reinforcement) to the king -&gt; crown and crown -&gt; power edges, and a positive delta (penalty) to the king -&gt; power edge.\nBehavioral Change: When the query was run again, the Reasoner, factoring in the new deltas from the Overlay, found that the path through crown was now the new cheapest path.\n\nThis demonstrates a complete, autonomous loop of Reason -&gt; Evaluate -&gt; Self-Reinforce -&gt; Reason Differently. The model improved its reasoning based on its own internal principles, a process far more akin to neuroplasticity than to traditional model training."
  },
  {
    "objectID": "02_Chronicle_P02_Engineering.html",
    "href": "02_Chronicle_P02_Engineering.html",
    "title": "Part 2: The Development Journey - From 6TB to 7.5GB",
    "section": "",
    "text": "Part 2: The Development Journey - From 6TB to 7.5GB\nThe ambitious vision of GDS required substantial engineering effort to become practically feasible. The development journey from theoretical concept to operational research prototype involved systematic optimization, documented in the project’s OPTIMIZATION_ROADMAP.md.\n\nThe Initial Challenge: The 5.95 TB Problem\nThe initial lexicon builder implementation faced a critical storage challenge. Using standard 64-bit float vectors (f64) for 20,000-dimensional HDC representations resulted in a projected storage requirement of 5.95 Terabytes for the full lexicon. This was computationally and financially infeasible on consumer hardware.\nThis constraint necessitated a fundamental re-evaluation of storage and data representation strategy, initiating a multi-stage optimization effort.\n\n\nStage 1: Binary HDC Implementation\nThe first major optimization aligned the implementation with HDC theoretical foundations. Canonical HDC research (Kanerva, Plate, et al.) uses binary or bipolar vectors rather than floating-point representations.\n\nThe Change: The pipeline was refactored to convert 300D Numberbatch embeddings into 20,000-bit binary vectors, packed densely into u64 words.\nThe Impact: This change reduced storage projection from 5.95 TB to 121 GB. Additionally, binary HDC operations (e.g., Hamming distance) proved computationally cheaper than floating-point operations, accelerating the build process by over 50%.\n\n\n\nStage 2: Columnar Storage with Parquet\nWhile 121 GB was feasible, further optimization was pursued. The second phase involved transitioning from binary/JSONL format to Apache Parquet, an industry-standard columnar storage format.\n\nThe Change: The ParticleWriter was rewritten using the arrow-rs library, implementing a proper columnar schema. This provided several advantages:\n\nColumnar Storage: Efficient for analytical queries and selective column access.\nZSTD Compression: Modern high-performance compression applied to all columns.\nNative Dictionary Encoding: Automatic dictionary encoding for high-cardinality string columns (lemma, concept_id), further reducing file size.\n\nThe Impact: This optimization yielded an additional ~3.5x compression ratio. The final storage requirement for a 3-million-particle lexicon reached 7.5 GB.\n\n\n\nSummary\nThrough this systematic, two-stage optimization process, an ~800x reduction in storage requirements was achieved, transforming an infeasible design into an operational research prototype. This development phase demonstrated that for exploratory research at this scale, aggressive optimization is essential rather than optional."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "GDS Deep Dive",
    "section": "",
    "text": "Welcome to the GDS Deep Dive\n\n\nThis site documents Geometrodynamic Semantics, a research exploration modeling meaning as a physical phenomenon. Follow the journey from concept to prototype as I explore physics-inspired reasoning and hyperdimensional computing.\n\n\nIndependent Research & Development Genesis Mihai A. Mateescu | mihai.mateescu@web.de\n\n\n\n\n\n\nAbout\n\n\nIndependent research initiative and collaboration opportunities.\n\n\n\n\n\n\nModel Card\n\n\nArchitecture, learning paradigm, and technical implementation.\n\n\n\n\n\n\nChronicle\n\n\nDevelopment journey from concept to validation.\n\n\n\n\n\n\nAnnexes\n\n\nCitations, optimization roadmaps, and documentation."
  },
  {
    "objectID": "02_Chronicle_P03_Simulation.html",
    "href": "02_Chronicle_P03_Simulation.html",
    "title": "Part 3: The Simulation - A Proof of Concept in Action",
    "section": "",
    "text": "Part 3: The Simulation - A Proof of Concept in Action\nTo validate the core principles of the GDS learning paradigm, a controlled experiment was designed and executed. The goal was not merely to test if the code ran, but to determine if the model could exhibit autonomous, non-trivial learning behavior.\n\n\n\n  \n    \n    \n      \n      king\n    \n    \n      \n      crown\n    \n    \n      \n      power\n    \n\n    \n    \n    \n    \n\n    Cost: 1.0\n    Cost: 0.8\n    Cost: 0.8\n  \n\n\n\n  \n    Step 1: Initial Reasoning\n    The Reasoner is asked to find the cheapest path from `king` to `power`. The direct path has a cost of 1.0, while the path through `crown` has a total cost of 1.6 (0.8 + 0.8). As expected, the model chooses the most direct and obvious route.\n  \n  \n    Step 2: Internal Evaluation & Learning\n    An internal heuristic evaluates the path through `crown` as being semantically richer. This triggers a learning event. A strong reinforcement (negative cost delta) is applied to the `king -&gt; crown` and `crown -&gt; power` edges, and a penalty (positive cost delta) is applied to the direct `king -&gt; power` edge.\n  \n  \n    Step 3: Verification & New Path\n    When the query is run again, the Reasoner factors in the new deltas from the Context Overlay. The path through `crown` is now the new cheapest path, and the model changes its preference, demonstrating a full, autonomous learning loop."
  },
  {
    "objectID": "02_Chronicle_P04_Evaluation.html",
    "href": "02_Chronicle_P04_Evaluation.html",
    "title": "Part 4: Evaluation & Research Directions",
    "section": "",
    "text": "Part 4: Evaluation & Research Directions\nHaving explored the GDS research from conceptual foundations through implementation and experimental validation, this section evaluates the prototype’s strengths, limitations, and potential research contributions.\n\nAssessment\nThe GDS prototype demonstrates feasibility of a physics-inspired approach to semantic reasoning. The implementation successfully translates abstract theoretical concepts into functional software, with experimental simulations confirming that core principles produce observable, emergent behaviors in semantic graph navigation.\n\n\nStrengths\n\nExplainability by Design: GDS’s primary advantage over mainstream LLMs is inherent interpretability. The PathExplain mechanism provides full transparency into reasoning processes. Each path includes costs and component contributions (mass, VAD, learning overlay), enabling both debugging and trust-building—a critical requirement for academic and scientific applications.\nAlternative Learning Paradigm: The autonomous learning loop based on internal evaluation and overlay modification offers a lightweight alternative to backpropagation. This approach aligns more closely with theories of neuroplasticity. Experimental simulations demonstrate that this mechanism successfully modifies preferential reasoning paths based on experience.\nCompositional Semantics: The typed HDC system with semantic roles (Subject, Object) enables compositional precision difficult to achieve in purely statistical models. This provides a framework for constructing complex semantic representations from simpler conceptual building blocks.\nEfficiency: The development journey from 6 TB projection to 7.5 GB implementation demonstrates architectural viability. Binary vectors, columnar storage (Parquet), and efficient indexing (FAISS) enable handling large-scale lexicons on consumer hardware.\n\n\n\nLimitations & Research Challenges\n\nParameter Complexity: Experimental simulations revealed that system behavior emerges from non-linear interactions among multiple components (cost weights, learning rates, diffusion, degree normalization). While this enables sophisticated reasoning, it complicates parameter tuning and reproducibility—a significant challenge for systematic research.\nData Dependency: Like knowledge-based systems generally, GDS inherits quality characteristics and biases from source data. Gaps or biases in ConceptNet, Numberbatch, or affective lexicons directly impact the Semantic Base. Systematic evaluation of data quality effects remains an open research question.\nHeuristic Components: Several components (coherence scoring, supertagger) currently use simple heuristics. Developing principled, data-driven methods for internal evaluation functions represents a key research direction for advancing beyond proof-of-concept.\n\n\n\nPotential for Academic Contribution\nThis research addresses several topics of interest to the AI/ML community:\n\nNovel Architecture: Presents a cognitive architecture contrasting with transformer-based approaches, offering an alternative paradigm for semantic reasoning.\nTheoretical Grounding: Integrates established theories from physics (geodesic motion in curved space), neuroscience (free energy principle, Hebbian learning), and computer science (hyperdimensional computing), providing interdisciplinary theoretical foundation.\nEmpirical Validation: Experimental simulations demonstrate core principles including autonomous learning and emergent path selection behavior. Complex, non-linear dynamics observed in the system provide material for analysis of emergent properties in semantic graphs.\nExplainability Focus: The emphasis on inherent interpretability addresses a key criticism of contemporary deep learning systems, making the work relevant to AI safety and trustworthiness research.\n\nA potential paper could structure the contribution as: (1) theoretical framework, (2) architectural implementation, (3) experimental validation through simulation, (4) analysis of emergent behaviors and non-linear dynamics.\n\n\nResearch Collaboration\nThis work represents independent research exploring alternative foundations for AI. I welcome collaboration with:\n\nAcademic researchers in AI, machine learning, neuroscience, or cognitive science\nResearch groups exploring physics-inspired or neurosymbolic approaches\nPotential PhD supervisors interested in novel AI paradigms\n\nThe project is conducted without institutional affiliation as part of Independent Research & Development Genesis. All code, documentation, and experimental results are openly documented to enable reproducibility and scholarly discussion.\nContact: mihai.mateescu@web.de\nIn summary, the GDS prototype validates a physics-inspired approach to semantic reasoning, demonstrating that alternative foundations for AI warrant further exploration and formal research investigation."
  },
  {
    "objectID": "01_ModelCard_GDS_Details.html#executive-summary-vision",
    "href": "01_ModelCard_GDS_Details.html#executive-summary-vision",
    "title": "Model Details",
    "section": "Executive Summary & Vision",
    "text": "Executive Summary & Vision\nGDS (Geometrodynamic Semantics) is a research prototype exploring an alternative to traditional, statistics-based Transformer architectures. Rather than predicting the next token, GDS models semantic reasoning as a physical phenomenon.\nInspired by Einstein’s theory of General Relativity, GDS treats concepts as “semantic particles” possessing intrinsic properties: mass (semantic importance), charge (the hyperdimensional vector), and spin (affective value). These particles are generated by the CSI-HDC (Conceptual State Injector using Hyperdimensional Computing)—a semantic tokenizer that replaces traditional token sequences with 20,000-dimensional binary hypervectors.\nThe CSI-HDC’s output is not a flat sequence of tokens, but a dynamic field of interacting particles. When processed by the GDS engine, this field warps a high-dimensional “conceptual space”. Reasoning is then modeled as finding the path of least resistance—a geodesic—through this curved semantic manifold.\nLearning occurs not through backpropagation, but through a Hebbian-style mechanism that modifies the geometry of the space itself. A dynamic Overlay layer adds contextual adjustments to edge costs in the graph. Successful reasoning paths are reinforced, making them “cheaper” and more likely in future queries. This process is governed by internal evaluation and a ValidationGate, enabling autonomous learning based on coherence principles rather than direct supervision.\nThe result is a research prototype demonstrating efficient, scalable, and—most importantly—explainable semantic reasoning, where every path can be audited and understood step-by-step."
  },
  {
    "objectID": "01_ModelCard_GDS_Details.html#detailed-architecture-how-the-system-reasons",
    "href": "01_ModelCard_GDS_Details.html#detailed-architecture-how-the-system-reasons",
    "title": "Model Details",
    "section": "Detailed Architecture (How the System Reasons)",
    "text": "Detailed Architecture (How the System Reasons)\n\n\n\nGDS Knowledge Graph Overview\n\n\nThe GDS cognitive architecture is a multi-layered research prototype where each layer has a distinct responsibility, from static data storage to dynamic, adaptive learning. The reasoning process emerges from the interaction of these layers.\n\nThe 5 Layers of GDS\n\nSemantic Base (The Static Universe)\n\nComponent: A large-scale, compressed Parquet file containing the lexicon of all “semantic particles”.\nRole: This is the foundational, long-term memory of the system. It contains millions of concepts, each with its pre-calculated mass (m0), affective spin (VAD), and a unique 20,000-bit HDC vector (q).\nIncludes: A static graph of structural edges derived from curated knowledge bases (e.g., ConceptNet’s IsA relation).\n\nProximity Graph (The Implicit Network)\n\nComponent: A graph layer constructed on top of the Semantic Base. Its crucial feature is the inclusion of proximity edges.\nRole: These edges are not explicit in the source data. They are discovered by performing a k-Nearest Neighbors (k-NN) search (using FAISS) on the HDC vectors. This allows the model to create novel connections between semantically similar concepts, even if they were not explicitly linked in any knowledge base. This graph represents the fabric of the “conceptual space”.\n\nContext Overlay (The Ephemeral Mind)\n\nComponent: A dynamic key-value store (LMDB) that maps graph edges to a delta value.\nRole: This is the model’s short-term, contextual memory. It holds temporary adjustments to the “cost” of traversing an edge. When the model learns, it doesn’t modify the static graph; it simply adds a small positive (penalty) or negative (reinforcement) delta to this overlay. It is volatile and session-specific by default.\n\nGeodesic Runtime (The “Thinker”)\n\nComponent: The Reasoner module, which implements a graph-traversal algorithm (A*).\nRole: This is the active part of the model. When given a start and a goal concept, the Reasoner does not just find the shortest path; it finds the path of least cost. The cost function is a sophisticated, weighted sum that makes the process “geodesic”:\n\nCost(edge) = a�(1/m0) + ߷(?VAD) + ?�(1/rel_strength) + ?�(Overlay_Delta)\n\nThis means the Reasoner naturally prefers paths that go through important concepts (high m0), avoid sharp emotional shifts (low ?VAD), follow strong structural relations, and are influenced by recent learning (Overlay_Delta).\n\nLearning Loop (The “Neuroplasticity”)\n\nComponent: The learn and gating modules.\nRole: This layer implements the model’s ability to adapt. After a reasoning task, an internal or external evaluation can trigger a learning event. The learn_edges function applies Hebbian-style updates to the Context Overlay. A ValidationGate then determines if these temporary changes have improved the model’s overall performance on a set of evaluation tasks before they are consolidated into a more permanent, versioned overlay.\n\n\n\n\nSummary of a “Thought”\nA GDS “thought” process can be summarized as:\n\nA query initiates a search for a low-cost path between two concepts in the Proximity Graph.\nThe Reasoner explores the graph, calculating the cost of each potential step using the multi-faceted cost function, which reads from both the static Semantic Base and the dynamic Context Overlay.\nThe resulting lowest-cost path is returned as the “thought” or solution.\nBased on the outcome, the Learning Loop can be triggered to update the Context Overlay, reinforcing or penalizing edges, thus altering the geometry of the space for the next, similar thought."
  },
  {
    "objectID": "01_ModelCard_GDS_Details.html#learning-paradigm",
    "href": "01_ModelCard_GDS_Details.html#learning-paradigm",
    "title": "Model Details",
    "section": "Learning Paradigm",
    "text": "Learning Paradigm\nThe GDS learning paradigm is fundamentally different from the backpropagation and gradient descent methods that power traditional Large Language Models. It is a form of autonomous, Hebbian-style learning that modifies the geometry of the conceptual space in response to experience.\n\nCore Principles\n\nNo Backpropagation: The model does not compute gradients across a massive neural network. Learning is a local, lightweight process.\nLearning by Modifying Costs: Instead of adjusting neuron weights, GDS learns by adjusting the “cost” of traversing specific edges in the semantic graph. This is done by writing small delta values to the dynamic Context Overlay.\nReinforcement and Penalization: Paths that lead to successful or “coherent” outcomes are reinforced (their edges receive a negative delta, making them cheaper and more attractive to the Reasoner). Paths that are evaluated as poor alternatives are penalized (their edges receive a positive delta, making them more expensive).\nInternal Evaluation: The model does not strictly require external, supervised labels to learn. As demonstrated in our simulation, it can employ internal heuristics (such as a “coherence score” based on concept mass) to decide which paths are “better” and thus worthy of reinforcement.\nStability and Explainability: Because learning only affects the overlay, the foundational knowledge graph remains stable. The changes are auditable (one can inspect the deltas in the overlay) and their effect is directly observable in the Reasoner’s behavior and cost calculations.\n\n\n\nCase Study: The Simulation\nOur simulation provided a perfect, concrete example of this paradigm in action:\n\nInitial State: The Reasoner initially chose the cheapest, most obvious path: king -&gt; power.\nInternal Evaluation: An internal metric, the “coherence score” (sum of concept masses), evaluated the alternative path king -&gt; crown -&gt; power as being semantically richer, despite its higher initial cost.\nAutonomous Learning: This internal evaluation triggered a learning event. The learn_edges function was called to apply a strong negative delta (reinforcement) to the king -&gt; crown and crown -&gt; power edges, and a positive delta (penalty) to the king -&gt; power edge.\nBehavioral Change: When the query was run again, the Reasoner, factoring in the new deltas from the Overlay, found that the path through crown was now the new cheapest path.\n\nThis demonstrates a complete, autonomous cycle: Reason → Evaluate → Self-Reinforce → Reason Differently. The system adapts its reasoning based on internal evaluation principles, a process resembling neuroplasticity more than traditional supervised learning."
  },
  {
    "objectID": "01_ModelCard_GDS_Details.html#data-lexicon-construction-csi-hdc",
    "href": "01_ModelCard_GDS_Details.html#data-lexicon-construction-csi-hdc",
    "title": "Model Details",
    "section": "Data & Lexicon Construction (CSI-HDC)",
    "text": "Data & Lexicon Construction (CSI-HDC)\nThe foundation of the GDS system is the Semantic Base, a large-scale lexicon of “semantic particles” generated by the CSI-HDC (Conceptual State Injector using Hyperdimensional Computing) pipeline. This pipeline processes and synthesizes information from multiple data sources to generate concept representations with physics-inspired properties.\n\nPrimary Data Sources\n\n\n\n\n\n\n\n\nData Source\nLocation in Project\nRole & Contribution\n\n\n\n\nConceptNet\ndata/raw/assertions.csv\nProvides the primary structural graph of common-sense relationships (e.g., UsedFor, CapableOf, PartOf). It forms the backbone of explicit knowledge.\n\n\nNumberbatch\ndata/raw/numberbatch.txt\nA set of pre-trained 300-dimensional word embeddings. It is the primary source for generating the 20,000-dimensional HDC vectors and serves as a fallback for calculating affective scores.\n\n\nNRC-VAD Lexicon\ndata/raw/nrc_vad/\nProvides affective scores for English words across three dimensions: Valence (pleasure/displeasure), Arousal (intensity), and Dominance (control). This is the source for the spin property of English particles.\n\n\nGerman Norms\ndata/raw/german_norms/\nThe German equivalent of the NRC-VAD lexicon, providing affective scores for German words.\n\n\nOEWM Lexicons\ndata/oewm_lexicons/\nOpen English, German, and Romanian WordNet data. This is a crucial source for normalization, synonymy (aliases), and word frequency priors. It significantly boosts the quality of mass calculation and the coverage of other lookups.\n\n\nBabelNet Cache\ndata/enrichment/babelnet_cache.db\nA local SQLite database that caches results from the BabelNet API. This is used in a daily enrichment loop to add new, high-quality multilingual relations to the graph, expanding its knowledge base over time.\n\n\n\n\n\nThe Generation Pipeline (LexiconBuilder)\nThe process is orchestrated by the LexiconBuilder in the Rust codebase and follows several key stages:\n\nAggregation: Raw assertions from ConceptNet are streamed and aggregated into a per-concept map, building a preliminary list of relations.\nNormalization & Enrichment: Lemmas are normalized using OEWM. This step also discovers aliases (synonyms) that will be used in later stages.\nQuality Scoring: Each potential concept is scored based on a set of heuristics: its connectivity in the graph, whether it has a Numberbatch embedding, and its coverage in affective lexicons.\nFiltering: Concepts that do not meet a minimum quality threshold (e.g., min_relations) are discarded.\nProperty Calculation: For each high-quality concept:\n\nMass (m0) is calculated based on its graph connectivity, boosted by its frequency from OEWM.\nSpin (s) is calculated from the affective lexicons (NRC-VAD, German Norms).\nCharge (q) is generated by passing its 300D Numberbatch embedding to the Julia HDC server, which expands it into a 20,000-bit binary hypervector.\n\nExport: The final collection of SemanticParticle objects is written to a compressed Parquet file, which becomes the Semantic Base for the GDS runtime."
  },
  {
    "objectID": "01_ModelCard_GDS_Details.html#evaluation-ethics-and-api",
    "href": "01_ModelCard_GDS_Details.html#evaluation-ethics-and-api",
    "title": "Model Details",
    "section": "Evaluation, Ethics, and API",
    "text": "Evaluation, Ethics, and API\n\nEvaluation\nEvaluation of the GDS model is two-fold, targeting both the quantitative performance of the system and the qualitative relevance of its reasoning.\n\nSystem Performance: As detailed in the OPTIMIZATION_ROADMAP.md, the lexicon construction pipeline is evaluated on metrics such as storage efficiency (bytes/particle), compression ratio, and throughput (particles/sec). The live system is evaluated on query latency and memory footprint.\nSemantic Quality: The quality of the model’s reasoning is evaluated through controlled tests. The simulation we performed is a prime example of a qualitative evaluation, designed to verify that the model’s behavior aligns with its core theoretical principles. Formal evaluation suites are planned to measure performance on tasks like:\n\nCross-lingual Retrieval: Testing if dog (en) is correctly identified as being close to Hund (de).\nGuided Analogy: Testing the quality of typed compositions (e.g., king - man + woman = queen).\nPath Coherence: Measuring the semantic consistency of paths found by the Reasoner.\n\n\n\n\nEthical Considerations & Social Impact\n\nTransparency & Explainability: A core design goal of GDS is to be explainable. Unlike the opaque nature of large transformer models, every step of a GDS “thought” process can be audited. The PathExplain object provides a full trace of the chosen path, the costs of each edge, and the contribution of each component (mass, VAD, overlay), making the model’s decisions transparent.\nBias Mitigation: The model’s knowledge is derived from its source data. While this data can contain biases, the GDS architecture offers several points of intervention. Telemetry tracks distributions per language and domain, allowing for monitoring of imbalances. The Overlay can also be used to apply targeted, corrective penalties to biased or undesirable associations in the graph.\nControl: The learning mechanism includes a ValidationGate, ensuring that autonomous changes to the Overlay are only consolidated after verifying that they do not degrade overall performance on a set of control tasks. This provides a crucial layer of human oversight and control over the model’s evolution.\n\n\n\nModel API\nThe GDS runtime exposes a clear, function-oriented API for interaction. The primary methods are:\n\nsearch_semantic(query, k): Performs a k-NN search in the HDC space to find the k concepts most similar to a query.\ncompose(vector_a, vector_b, operation): Creates a new concept by composing two existing vectors using typed HDC operations.\nreason(start_concept, goal_concept, constraints): The core function. It invokes the Reasoner to find the lowest-cost path between two concepts, returning the full PathExplain object.\nlearn(path, negatives): Triggers a learning event. It takes a path to be reinforced and a set of negative edges to be penalized, which then updates the Context Overlay."
  },
  {
    "objectID": "GDS_Concept/lexicon_audit.html#pipeline-coverage-review",
    "href": "GDS_Concept/lexicon_audit.html#pipeline-coverage-review",
    "title": "Lexicon Data Usage Audit (2025-10)",
    "section": "1. Pipeline Coverage Review",
    "text": "1. Pipeline Coverage Review\n\nConceptNet assertions feed the structural graph via ConceptNetParser::process_in_batches. All RO/DE/EN edges are aggregated into ConceptAccumulator, with per-edge weights and max-relations trimming (default 64).\nOEWM trilingual lexicon is loaded once (OewmLexicon::load_dir). The pipeline currently uses it for:\n\nlemma normalization during aggregation;\nalias discovery for Numberbatch/VAD fallbacks;\nOEWM frequency priors in SemanticParticle::calculate_mass;\nsupertagging hints in the HDC layer (Supertagger::infer).\n\nNumberbatch embeddings (NumberbatchDB) provide the mandatory 300D vectors. Alias lookups fall back to OEWM-normalized forms.\nAffective lexica:\n\nNRC VAD v2 powers English spin projections and the global spin axes when available;\nIMS German norms supply DE affective scores; alias lookups mirror the embedding logic.\n\nBabelNet cache enrichment is available via BabelNetCacheReader, injecting additional edges (weight 1.2) when lemma-level matches succeed.\nCheckpoint + telemetry: enrichment, alias hits, and KO statistics are tracked, but not persisted into the final Parquet artifact.\n\nOverall, each external source is wired into the build, but several signals are downgraded or dropped before the Parquet export."
  },
  {
    "objectID": "GDS_Concept/lexicon_audit.html#gaps-inefficiencies-identified",
    "href": "GDS_Concept/lexicon_audit.html#gaps-inefficiencies-identified",
    "title": "Lexicon Data Usage Audit (2025-10)",
    "section": "2. Gaps & Inefficiencies Identified",
    "text": "2. Gaps & Inefficiencies Identified\n\nLoss of provenance in output – the Parquet schema omits canonical IDs, alias lists, BabelNet IDs, frequency priors, and relation payloads. Downstream consumers cannot reconstruct cross-ling synsets or inspect which sources contributed to each particle.\nCanonical consolidation missing – OEWM exposes canonical_id, yet the aggregator never merges ConceptNet nodes that map to the same canonical concept. Cross-language variants continue to exist as independent particles, losing the opportunity to surface multilingual bundles.\nRelation archive absent – SemanticParticle.relations is populated (including BabelNet deltas) but discarded when writing Parquet (relations: Vec::new() in ParticleWriter). The runtime must rebuild adjacency from ConceptNet on every load, negating the enrichment step.\nBabelNet lookup fragile – BabelNetCacheReader::get_by_lemma uses format!(\"{lang}:{lemma}\"). Because the lemma is OEWM-normalised (underscores, lower-case), any mismatch with the cache’s canonical form breaks enrichment. No fallback tries concept URIs or canonical IDs.\nFrequency prior asymmetry – only German frequency tables are shipped (DecoW14/16). EN and RO particles never benefit from OEWM priors, skewing mass calibration toward German entries.\nLanguage quota heuristics – QualityFilter::score adds a constant 1 / max_ratio boost for any tracked language, regardless of current coverage. This inflates scores even when a language is already overrepresented and partially cancels the quota guard.\nAlias telemetry not persisted – telemetry records alias-assisted embeddings/VAD hits, but this metadata is not exported alongside particles, preventing ex-post quality analysis."
  },
  {
    "objectID": "GDS_Concept/lexicon_audit.html#recommended-actions",
    "href": "GDS_Concept/lexicon_audit.html#recommended-actions",
    "title": "Lexicon Data Usage Audit (2025-10)",
    "section": "3. Recommended Actions",
    "text": "3. Recommended Actions\n\nExtend output schema to include provenance columns:\n\ncanonical_id, alias_best, alias_all, frequency_prior, embedding_source, affective_source, babelnet_id, source_flags.\nstore vector_binary as fixed-size binary; consider optional vector_hash for integrity checks.\n\nEmit relation table (edges.parquet) with fields source_particle, target_particle, relation_type, weight, origin. Reference by integer particle IDs to keep Parquet lean.\nCanonical compaction:\n\ngroup ConceptNet nodes by OEWM canonical ID during aggregation;\nmaintain per-canonical multilingual lemma bundles with language tags.\n\nBabelNet key expansion:\n\nattempt cache.get(concept_id) before lemma lookup;\ntry OEWM canonical IDs as secondary keys;\nrecord enrichment success/failure in the particle metadata.\n\nFrequency coverage:\n\ningest English (SUBTLEX, wordfreq) and Romanian (CoRoLa, RoWaC) frequency tables into OEWM builder;\nexpose language-specific priors for mass calibration.\n\nQuota-aware scoring:\n\nreplace the static score boost with a dynamic factor based on (target_ratio - current_ratio), clamped to ±1.0.\n\nPersist telemetry snapshots for each build (JSON co-located with Parquet) to inform regression monitoring."
  },
  {
    "objectID": "GDS_Concept/lexicon_audit.html#proposed-enriched-particle-record-key-fields",
    "href": "GDS_Concept/lexicon_audit.html#proposed-enriched-particle-record-key-fields",
    "title": "Lexicon Data Usage Audit (2025-10)",
    "section": "4. Proposed Enriched Particle Record (key fields)",
    "text": "4. Proposed Enriched Particle Record (key fields)\n\n\n\n\n\n\n\n\nField\nSource\nNotes\n\n\n\n\nparticle_id\npipeline\nSequential row ID\n\n\nconcept_uri\nConceptNet\nOriginal /c/{lang}/{lemma} URI\n\n\ncanonical_id\nOEWM\nNormalised synset identifier (e.g., oewn:ENG30-10225334-n)\n\n\nlemmas\nOEWM + ConceptNet\nMap of language → preferred lemma\n\n\naliases\nOEWM\nSame-language variants used for fallbacks\n\n\nlanguages_available\nOEWM\nLanguages sharing the canonical synset\n\n\nmass\nConceptNet + OEWM\nIncludes frequency prior boost\n\n\nfrequency_prior\nOEWM\nNormalised [0,1]\n\n\nspin\nNRC VAD / IMS norms\nStructure: {source, valence, arousal, dominance}\n\n\nembedding\nNumberbatch\n{source: direct/alias, term}\n\n\nvector_binary\nJulia HDC\nFixed 2,500-byte blob\n\n\nvector_hash\nJulia HDC\n64-bit checksum for validation\n\n\nrelations\nConceptNet + BabelNet\nArray of {type, target_uri, weight, origin}\n\n\nbabelnet\nBabelNet cache\n{id, synset_type, lemmas, glosses}\n\n\nsource_flags\npipeline\ne.g., [\"conceptnet\", \"numberbatch\", \"vad\", \"babelnet\"]\n\n\n\nAn example payload reflecting the schema is provided in lexicon_enriched_sample.json. This structure captures the contributions of every upstream resource while remaining compatible with Parquet/Arrow exports (nested lists → struct arrays)."
  },
  {
    "objectID": "GDS_Concept/lexicon_audit.html#implementation-status-2025-10-xx",
    "href": "GDS_Concept/lexicon_audit.html#implementation-status-2025-10-xx",
    "title": "Lexicon Data Usage Audit (2025-10)",
    "section": "5. Implementation Status (2025-10-XX)",
    "text": "5. Implementation Status (2025-10-XX)\n\n✅ Schema uplift completed – Parquet rows now emit canonical IDs, alias bundles, provenance flags, BabelNet IDs, and relation JSON payloads.\n✅ Canonical/alias capture – ConceptNet aggregation records OEWM canonical IDs and alias sets; telemetry reports alias-assisted embeddings/VAD.\n✅ BabelNet fallback logic – Cache lookups now probe ConceptNet URIs, canonical IDs, and aliases before giving up.\n✅ Telemetry v2 – Build reports include schema version gds_particle_v2, BabelNet coverage counts, and enriched provenance metrics.\n✅ Frequency loader generalized – OEWM frequency tables accept optional EN/RO dumps in addition to the German defaults.\n✅ Documentation refresh – CLAUDE.md, Quarto annexes, and the citations page reference the new schema and roadmap.\n✅ Gold dataset sample – Synthetic best-case snapshot published at docs/GDS_Deep_Dive/tests/simulation/gold_dataset_sample.jsonl for simulation replays.\n⏳ Canonical compaction – Cross-language particle merging scheduled for the next iteration once evaluation harnesses are updated."
  },
  {
    "objectID": "GDS_Concept/simulation_report.html#phase-1-synthetic-world-generation",
    "href": "GDS_Concept/simulation_report.html#phase-1-synthetic-world-generation",
    "title": "GDS Simulation Report",
    "section": "Phase 1: Synthetic World Generation",
    "text": "Phase 1: Synthetic World Generation\n\nSynthetic world schema loaded.\nGenerated 14 semantic particles in memory."
  },
  {
    "objectID": "GDS_Concept/simulation_report.html#phase-2-graph-construction-reasoning",
    "href": "GDS_Concept/simulation_report.html#phase-2-graph-construction-reasoning",
    "title": "GDS Simulation Report",
    "section": "Phase 2: Graph Construction & Reasoning",
    "text": "Phase 2: Graph Construction & Reasoning\n\nSemantic graph built successfully with 14 nodes.\n\n\nStep 1: Initial Reasoning Query\n\nQuery: Find the cheapest path from ‘king’ to ‘power’ with an empty Overlay.\nResult: Path \"king -&gt; power\" found.\nInitial Cost: 1.5102\n\nInitial Cost Breakdown: inv_m0=0.9102, vad=0.0000, rel=0.6000, overlay=0.0000\n\n\n\n\nStep 2: Internal Coherence Scoring\n\nGoal: Evaluate which path is semantically richer based on the sum of concept mass (m0).\nCoherence score for king -&gt; power: 2.8904\nCoherence score for king -&gt; crown -&gt; power: 3.9890\nConclusion: The path through ‘crown’ is considered semantically richer.\n\n\n\nStep 3: Autonomous Learning Event\n\nAction: Reinforce the richer path and penalize the alternative.\nLearning Parameters: eta_pos (reinforcement) = -0.5, eta_neg (penalty) = 0.01, post_diffusion = disabled.\nOutcome: 3 edges were updated in the Overlay.\n\n\n\nStep 4: Verification Query\n\nQuery: Rerun the exact same query (king -&gt; power) on the modified Overlay.\nResult: Path \"king -&gt; power\" found.\nFinal Cost: 1.4879\n\nFinal Cost Breakdown: inv_m0=0.9102, vad=0.0000, rel=0.6000, overlay=-0.0223\n\n\n\n\nStep 5: Final Analysis\n\nObservation: The cost of the direct path king -&gt; power changed from 1.5102 to 1.4879.\nAnalysis: The penalty delta of +0.01 was applied to this edge.\nHowever, the overlay_contrib is normalized: raw_delta / sqrt(degree_out * degree_in).\nDegrees: out(king)=5, in(power)=2. Denominator = 3.1623.\nThe final overlay contribution was -0.0223, which was not enough to change the outcome.\n\nConclusion: The model did not change its path. This reveals the critical importance of the degree_normalization in the cost function and shows that learning is non-linear. A small penalty on a highly connected edge can be insignificant. This is a key insight into the model’s behavior."
  },
  {
    "objectID": "GDS_Concept/overview.html#system-design-philosophy",
    "href": "GDS_Concept/overview.html#system-design-philosophy",
    "title": "GDS Technical Architecture",
    "section": "System Design Philosophy",
    "text": "System Design Philosophy\nGDS models semantic reasoning as a physics-inspired process operating over a hyperdimensional concept space. Rather than treating language as statistical patterns, the system represents meaning through:\n\nSemantic particles: Concepts encoded as 20,000-dimensional binary HDC vectors\nGeometric relationships: Knowledge graph edges with physics-inspired properties (mass, valence, affective dimensions)\nGeodesic reasoning: Path-finding through semantic space using composite cost functions"
  },
  {
    "objectID": "GDS_Concept/overview.html#core-architecture-layers",
    "href": "GDS_Concept/overview.html#core-architecture-layers",
    "title": "GDS Technical Architecture",
    "section": "Core Architecture Layers",
    "text": "Core Architecture Layers\n\n1. Semantic Base Layer (Static)\nThe foundational lexicon built from multiple knowledge sources:\n\nStorage Format: Parquet with ZSTD compression (~7.5GB for 340k concepts)\nConceptual State Injector (CSI-HDC): Replaces traditional tokenization with 20,000-dimensional binary hypervectors\nVector Expansion: Julia HDC server generates concept hypervectors from 300D embeddings\nGraph Structure: Curated assertions (ConceptNet) + proximity edges (k-NN via FAISS binary indexing)\n\nKey Properties Per Concept:\n\nparticle_id, concept_id, lemma, language\nm0 (semantic mass/centrality)\nVAD (Valence-Arousal-Dominance) affective dimensions\n20,000-bit binary HDC vector\nGraph connectivity (edges with relation types)\n\n\n\n2. Dynamic Context Overlay (Runtime)\nLMDB-backed delta store for runtime learning:\n\nEdge weight adjustments during reasoning sessions\nDegree-normalized updates to maintain graph stability\nCheckpointing for session recovery\nTelemetry monitoring (drift detection, margin analysis)\n\n\n\n3. Reasoning Engine\nPath-based semantic reasoning through the concept graph:\n\nGeodesic solver: Multi-component cost function combining:\n\nα·inv_m0: Semantic mass preference (favor central concepts)\nβ·vad: Affective dimension matching\nγ·rel: Relation type appropriateness\nλ·overlay: Runtime learned preferences\n\nExplainability: JSON traces (gds.explain.path@v1) documenting:\n\nTraversed nodes and edges\nCost breakdown per component\nReasoning margin (confidence)\n\n\n\n\n4. Learning Subsystem\nPhysics-inspired autonomous learning:\n\nLocal Hebbian updates: Edge strengthening based on usage patterns\nValidation gates: Telemetry-based checks before consolidating learned changes\nDiffusion smoothing: L2 decay prevents overfitting to specific paths\nMargin-gain optimization: Focus learning on high-uncertainty decisions"
  },
  {
    "objectID": "GDS_Concept/overview.html#technology-stack",
    "href": "GDS_Concept/overview.html#technology-stack",
    "title": "GDS Technical Architecture",
    "section": "Technology Stack",
    "text": "Technology Stack\n\n\n\n\n\n\n\n\nComponent\nTechnology\nPurpose\n\n\n\n\nCore Logic\nRust\nReasoning engine, graph operations, I/O\n\n\nVector Expansion\nJulia\nHDC hypervector generation from embeddings\n\n\nStorage\nParquet + ZSTD\nCompressed columnar lexicon storage\n\n\nOverlay\nLMDB\nRuntime delta store with checkpointing\n\n\nIndexing\nFAISS (binary)\nk-NN similarity search for graph construction\n\n\nFFI\nJSON over stdin/stdout\nRust ↔︎ Julia communication"
  },
  {
    "objectID": "GDS_Concept/overview.html#data-sources",
    "href": "GDS_Concept/overview.html#data-sources",
    "title": "GDS Technical Architecture",
    "section": "Data Sources",
    "text": "Data Sources\nThe research prototype integrates multiple knowledge bases:\n\nConceptNet 5: ~2.1M curated semantic assertions (140k concepts after filtering)\nConceptNet Numberbatch: 300D multilingual embeddings\nNRC-VAD Lexicon: Affective dimensions for emotional reasoning\nOEWN (Open English WordNet): Lexical taxonomy and relationships"
  },
  {
    "objectID": "GDS_Concept/overview.html#implementation-highlights",
    "href": "GDS_Concept/overview.html#implementation-highlights",
    "title": "GDS Technical Architecture",
    "section": "Implementation Highlights",
    "text": "Implementation Highlights\n\nStorage Optimization Journey\nThe project achieved ~800x storage reduction through progressive optimization:\n\nInitial projection: 6TB for full lexicon (300D floats + metadata)\nBinary HDC encoding: Reduced to ~68GB (20k-bit vectors)\nParquet columnar storage: Final ~7.5GB (with ZSTD compression)\n\n\n\nCSI-HDC Tokenization\nConceptual State Injector using Hyperdimensional Computing (CSI-HDC) replaces traditional neural tokenizers:\n\nInput: Concept identifier (e.g., \"en/cat\", \"ro/calculator\")\nProcess: Julia HDC server expands 300D embedding → 20,000D binary vector\nOutput: Semantic particle ready for graph-based reasoning\nAdvantage: Interpretable, multilingual, compositional\n\n\n\nCross-Language Capabilities\nThe system demonstrates multilingual semantic reasoning:\n\nEnglish: Primary language (140k concepts)\nRomanian: Secondary language (~200k concepts)\nShared HDC space: Cross-lingual concept alignment via Numberbatch embeddings\nRelation preservation: Language-agnostic graph structure"
  },
  {
    "objectID": "GDS_Concept/overview.html#current-research-status",
    "href": "GDS_Concept/overview.html#current-research-status",
    "title": "GDS Technical Architecture",
    "section": "Current Research Status",
    "text": "Current Research Status\nGDS is a proof-of-concept prototype demonstrating:\n✅ Operational CSI-HDC tokenization (20k-dimensional binary vectors)\n✅ Functional graph-based reasoning with explainable paths\n✅ Autonomous learning gates with validation\n✅ Efficient storage (Parquet + LMDB architecture)\n✅ Multilingual capabilities (EN/RO)\nThis architecture represents an exploration of physics-inspired semantic reasoning, not a production system. All design decisions prioritize research interpretability and experimental flexibility."
  },
  {
    "objectID": "GDS_Concept/overview.html#further-reading",
    "href": "GDS_Concept/overview.html#further-reading",
    "title": "GDS Technical Architecture",
    "section": "Further Reading",
    "text": "Further Reading\n\nModel Card: Detailed specifications and capabilities\nDevelopment Chronicle: Engineering journey and optimizations\nCitations: Academic references and data sources\n\n\nThis architecture serves the Independent Research & Development Genesis initiative exploring novel AI paradigms. For questions or collaboration: mihai.mateescu@web.de"
  },
  {
    "objectID": "GDS_Concept/CLAUDE.html#project-identity",
    "href": "GDS_Concept/CLAUDE.html#project-identity",
    "title": "CLAUDE.md",
    "section": "1. Project Identity",
    "text": "1. Project Identity\n\nName: GDS – Geometrodynamic Semantics\nMission: Build a continuously learning multilingual semantic lexicon using hyperdimensional computing and physics-inspired semantics (concepts have mass m₀, charge q, spin s).\nLanguages: Romanian (RO), English (EN), German (DE). Additional languages must only be added after extending all pipelines and lexicons.\nStatus (2025‑10‑06): Production pipeline live. End-to-end Rust + Julia build with Parquet output and daily BabelNet enrichment. Current emphasis: scaling volume while preserving quality."
  },
  {
    "objectID": "GDS_Concept/CLAUDE.html#system-architecture",
    "href": "GDS_Concept/CLAUDE.html#system-architecture",
    "title": "CLAUDE.md",
    "section": "2. System Architecture",
    "text": "2. System Architecture\n\n2.1 High-level topology\n┌─────────────────────────────────────────────────────────┐\n│ Rust crate (gds_core)                                   │\n│  • Streams ConceptNet assertions                         │\n│  • Loads Numberbatch, NRC VAD, German norms              │\n│  • Scores concepts (quality heuristics)                  │\n│  • Calls Julia HDC server for 20k-bit vectors            │\n│  • Writes Parquet via Arrow + ZSTD                      │\n└───────────▲──────────────────────────────────────────────┘\n            │ Zero-copy FFI (cdylib)\n┌───────────┴──────────────────────────────────────────────┐\n│ Julia GDSCore                                            │\n│  • SIMD HDC ops (LoopVectorization)                      │\n│  • Random projection + binarization                      │\n│  • Spin fallback from embeddings                         │\n│  • Persistent stdin/stdout server (`julia/hdc_server.jl`)│\n└──────────────────────────────────────────────────────────┘\n\n\n2.2 Key Rust modules\n\n\n\n\n\n\n\nPath\nResponsibility\n\n\n\n\nsrc/lib.rs\nDefines SemanticParticle, LexiconBuilder, concept aggregation, quality pipeline, Julia integration, Parquet export\n\n\nsrc/quality/mod.rs\nHeuristics (min relations, embedding/VAD requirements, soft language ratios, particle caps)\n\n\nsrc/quality/telemetry.rs\nSelection telemetry (rejection reasons, JSON reporting)\n\n\nsrc/parsers/conceptnet.rs\nStreams TSV/TSV.GZ assertions in batches; resolves columns; language filter\n\n\nsrc/storage/parquet_writer.rs\nSchema: particle_id, id, concept_id, lemma, language, mass, spin_*, vector_binary. Tracks metadata and file size\n\n\nsrc/storage/checkpoint.rs\nLMDB-backed checkpoint coordinator (aggregation resume, particle persistence, Parquet replay metadata)\n\n\nsrc/ffi/julia_hdc.rs\nPersistent Julia process management, zero-copy interfaces\n\n\nsrc/main.rs\nCLI orchestration with clap; merges config + CLI overrides\n\n\nexamples/*\nDiagnostics (parquet quality analysis, enrichment tools, integration tests)\n\n\n\n\n\n2.3 Key Julia files\n\n\n\n\n\n\n\nFile\nNotes\n\n\n\n\njulia/src/HDCCore.jl\nSIMD bind/bundle/permute; calculate_spin falls back to embedding segments when VAD neutral\n\n\njulia/src/RustFFI.jl\nunsafe_wrap zero-copy access to Rust memory\n\n\njulia/hdc_server.jl\nPersistent server returning 2,500 binary bytes per request"
  },
  {
    "objectID": "GDS_Concept/CLAUDE.html#data-assets",
    "href": "GDS_Concept/CLAUDE.html#data-assets",
    "title": "CLAUDE.md",
    "section": "3. Data Assets",
    "text": "3. Data Assets\n\n\n\n\n\n\n\n\nData\nLocation\nUsage\n\n\n\n\nConceptNet assertions\ndata/raw/assertions.csv\nPrimary relation graph (up to 34M assertions)\n\n\nNumberbatch 300D\ndata/raw/numberbatch.txt\nEmbeddings for require_embedding and spin fallback\n\n\nNRC VAD\ndata/raw/nrc_vad/NRC-VAD-Lexicon-v2.1/...\nEN affective scores\n\n\nGerman norms\ndata/raw/german_norms/de_emo_norms.txt\nDE affective scores (9 dimensions)\n\n\nBabelNet cache\ndata/enrichment/babelnet_cache.db\nSQLite cache filled daily (enrichments consume BabelCoins)\n\n\n\n\nReminder: All lookups assume UTF-8, lowercase lemmas, _ for spaces. Maintain consistent preprocessing when new resources are added."
  },
  {
    "objectID": "GDS_Concept/CLAUDE.html#build-execution",
    "href": "GDS_Concept/CLAUDE.html#build-execution",
    "title": "CLAUDE.md",
    "section": "4. Build & Execution",
    "text": "4. Build & Execution\n\n4.1 CLI (primary entry point)\ncargo run -- --config config/lexicon_build.toml [options]\nImportant options:\n\n--max-assertions N: stream only first N ConceptNet assertions (use for dry runs).\nOverride paths: --conceptnet, --numberbatch, --vad, --german-norms, --babelnet-cache, --output.\nQuality overrides: --min-relations, --require-embedding, --require-vad, --target-particles, --max-relations-per-concept.\n\nRelease build for production:\ncargo run --release -- --config config/lexicon_build.toml\nCargo automatically rebuilds the crate when sources change; no manual rebuild step is required.\n\n\n4.2 Configuration (config/lexicon_build.toml)\nconceptnet_path = \"data/raw/assertions.csv\"\nnumberbatch_path = \"data/raw/numberbatch.txt\"\nvad_path = \"data/raw/nrc_vad/.../NRC-VAD-Lexicon-v2.1.txt\"\ngerman_norms_path = \"data/raw/german_norms/de_emo_norms.txt\"\nbabelnet_cache_path = \"data/enrichment/babelnet_cache.db\"\noutput_path = \"data/processed/full_lexicon.parquet\"\n\nmin_relations = 3\nrequire_embedding = true\nrequire_vad = false\n# 0 =&gt; unlimited in code\ntarget_particles = 0\nmax_relations_per_concept = 64\n\n[target_language_ratio]\nen = 0.60\nde = 0.50\n# ro omitted intentionally (no cap)\n\n\n4.3 Pipeline stages (Rust LexiconBuilder::build)\n\nLoad resources: Numberbatch (full), NRC VAD, German norms, BabelNet cache (if existing). Build affective axes from high/low VAD embeddings.\nStream ConceptNet: ConceptNetParser::process_in_batches collapses assertions into per-concept accumulators, deduping edges and recording weights.\nQuality scoring: ConceptQualitySnapshot computed for each concept, scoring by relation count, embedding/VAD/BabelNet coverage. Soft language ratios include tolerance that shrinks as dataset grows.\nSort & filter: Candidates sorted by score. QualityFilter::evaluate returns accept/reject + reason (telemetry); ROMANIAN has no upper cap by default.\nEnrichment merge: BabelNet relations appended with weight 1.2 when cache entry exists.\nMass & spin: calculate_mass(frequency_prior) scales ln(1 + relation_count) × average weight with OEWM frequency boosts before assigning spins.\nJulia HDC: JuliaHDC::expand_to_20kd communicates with persistent Julia server. Non-embedded concepts fall back to zero vector (keeps density at ~0.5).\nParquet export: ParticleWriter::write_particle buffers into batches (1k). Metadata collects unique counts and file size. The enriched schema now includes provenance columns (canonical_id, alias_primary, alias_candidates_json, frequency_prior, embedding_source, affective_source, babelnet_id, source_flags_json, relations_json) beside the numeric mass/spin/vector fields.\nReport: CLI prints number of concepts aggregated, candidate summary (percentage with embeddings/VAD/BabelNet), particles written, coverage, language distribution.\n\n\nObservation: With min_relations = 3 and require_embedding = true, expect roughly 60–70% of candidates to remain; VAD coverage is ~35–45% on ConceptNet subsets. Tune thresholds accordingly."
  },
  {
    "objectID": "GDS_Concept/CLAUDE.html#quality-heuristics-monitoring",
    "href": "GDS_Concept/CLAUDE.html#quality-heuristics-monitoring",
    "title": "CLAUDE.md",
    "section": "5. Quality Heuristics & Monitoring",
    "text": "5. Quality Heuristics & Monitoring\n\n\n\n\n\n\n\n\nLever\nEffect\nTypical Setting\n\n\n\n\nmin_relations\nMinimum unique relations to keep concept\n3 (raise to 4–5 for higher precision)\n\n\nrequire_embedding\nDiscard concepts lacking Numberbatch vector\ntrue in production\n\n\nrequire_vad\nForce VAD coverage\nfalse (use spin fallback)\n\n\ntarget_particles\nSoft cap. 0 means unlimited\nUse for staged builds (e.g., 100_000)\n\n\ntarget_language_ratio\nSoft cap per language\nAdjust to steer distribution; tolerance auto-adjusted\n\n\nmax_relations_per_concept\nPrevent huge adjacency lists\n64 (tune to reduce Parquet size)\n\n\n\nDiagnostic command after build:\ncargo run --example analyze_lexicon_quality -- data/processed/full_lexicon.parquet\nPrints total particles, per-language counts, mass percentiles, bit density, and valence distribution."
  },
  {
    "objectID": "GDS_Concept/CLAUDE.html#daily-enrichment-loop",
    "href": "GDS_Concept/CLAUDE.html#daily-enrichment-loop",
    "title": "CLAUDE.md",
    "section": "6. Daily Enrichment Loop",
    "text": "6. Daily Enrichment Loop\n\nscripts/daily_enrichment.sh\n\nSources .config/babelnet.env (must contain export BABELNET_API_KEY=\"...\").\nExecutes cargo run --release --example enrich_daily_batch (adaptive batch size, dedupes using cache, stops when BabelCoins low).\nOn success: cargo run --release -- --config config/lexicon_build.toml to regenerate Parquet.\nWeekly Sunday backups compressed (gzip), 8-week retention.\n\nscripts/setup_cron.sh installs a cron entry at 02:00 daily. Re-run script after editing the shell script.\nPriority queue builder (examples/build_enrichment_queue.rs) ranks top BabelNet targets. Ensure it is refreshed periodically if ConceptNet scoring parameters change."
  },
  {
    "objectID": "GDS_Concept/CLAUDE.html#testing",
    "href": "GDS_Concept/CLAUDE.html#testing",
    "title": "CLAUDE.md",
    "section": "7. Testing",
    "text": "7. Testing\n\ncargo test --lib & cargo test are required to pass. Julia integration test auto-skips (with warning) if Julia executable/config is missing—never ignore failures silently.\nIntegration checks:\n\ncargo run --example test_parquet_writer\ncargo run --example test_julia_integration\ncargo test --test mini_build (mini pipeline with mock Julia + fixtures)\ncargo run --example analyze_lexicon_quality -- &lt;parquet&gt;\n\nBefore large builds, run cargo check --release to catch unused imports/typos."
  },
  {
    "objectID": "GDS_Concept/CLAUDE.html#outstanding-tasks-future-work",
    "href": "GDS_Concept/CLAUDE.html#outstanding-tasks-future-work",
    "title": "CLAUDE.md",
    "section": "8. Outstanding Tasks / Future Work",
    "text": "8. Outstanding Tasks / Future Work\n\nScaling to full dataset: run without --max-assertions, monitor memory (&lt;30 GB target). Consider LMDB checkpointing if builds exceed 6h.\nEnhanced heuristics: mass percentile filters, BabelNet importance weighting, prioritized sampling of underrepresented languages.\nJulia server telemetry: detect and rotate on memory bloat, capture expansion timings.\nVAD augmentation: add RO affective resources or translation heuristics.\nAutomated QA dashboards: ingest Parquet stats into monitoring pipeline (future).\n\nDocument any new initiatives in dedicated docs/*.md annexes, referencing them from this file."
  },
  {
    "objectID": "GDS_Concept/CLAUDE.html#agent-operating-rules-non-negotiable",
    "href": "GDS_Concept/CLAUDE.html#agent-operating-rules-non-negotiable",
    "title": "CLAUDE.md",
    "section": "9. Agent Operating Rules (Non-negotiable)",
    "text": "9. Agent Operating Rules (Non-negotiable)\n\nQuality First: zero tolerance for mock-ups, stubs, placeholder logic, hard-coded demos, commented-out code to “keep for later,” or partially implemented features left surfaced.\nSequential Reasoning: before implementing any complex or novel feature, run the sequential-thinking MCP tool to outline approach, and consult the web search MCP for external references as needed.\nDocumentation: create/append an annex (e.g., docs/&lt;topic&gt;.md) for any significant change, summarizing research and decisions. Reference the annex from this file when merging major features.\nNo Warning Suppression: compiler warnings or runtime errors must be resolved through proper fixes, never hidden or bypassed with dummy values or allow attributes.\nTesting Discipline: run cargo fmt, cargo check, and relevant tests before marking work complete. For Julia-dependent code, ensure the integration test path succeeds or explicitly documents environment limitations.\nConfig Consistency: if configuration schema changes, update config/lexicon_build.toml, this file, and the CLI parser simultaneously.\nDependency Changes: record library upgrades or new dependencies in a short docs/changes/&lt;date&gt;_dependency_update.md and update this file’s relevant sections.\nReview Mindset: treat every code change as production-grade. Evaluate readability, performance, and scalability. If uncertain, stop and research before coding.\nCommunication: if encountering ambiguous requirements, pause and clarify through repo notes or direct questions—no assumptions.\n\nThese rules apply to all AI agents and human contributors. Violations must be corrected immediately."
  },
  {
    "objectID": "GDS_Concept/CLAUDE.html#quick-links-commands",
    "href": "GDS_Concept/CLAUDE.html#quick-links-commands",
    "title": "CLAUDE.md",
    "section": "10. Quick Links / Commands",
    "text": "10. Quick Links / Commands\n\nBuild (dev): cargo run -- --config config/lexicon_build.toml --max-assertions 100000\nBuild (release): cargo run --release -- --config config/lexicon_build.toml\nDaily run (manual): ./scripts/daily_enrichment.sh\nAnalyze output: cargo run --example analyze_lexicon_quality -- data/processed/full_lexicon.parquet\nTelemetry summary: cargo run --example telemetry_summary -- [logs/build_telemetry.json]\nLint/tests: cargo fmt && cargo check && cargo test --lib\nTelemetry JSON: logs/build_telemetry.json\nPoC injector: cargo run --example poc_semantic_injector -- data/processed/full_lexicon.jsonl --limit 20000 sun light energy"
  },
  {
    "objectID": "GDS_Concept/CLAUDE.html#current-proof-of-concept-status-next-steps",
    "href": "GDS_Concept/CLAUDE.html#current-proof-of-concept-status-next-steps",
    "title": "CLAUDE.md",
    "section": "12. Current Proof-of-Concept Status & Next Steps",
    "text": "12. Current Proof-of-Concept Status & Next Steps\n\nAchieved (2025-10-06)\n\nLMDB checkpoints operational (GDS_CHECKPOINT_DIR override) + selection telemetry JSON (alias-assisted metrics, margin) written to logs/build_telemetry.json.\nGDS_MOCK_JULIA=1 mock backend delivers deterministic hypervectors; production uses the persistent Julia server (julia/hdc_server.jl).\nMini integration test (cargo test --test mini_build) covers the aggregation → selection → Parquet → telemetry flow.\nPoC injector (examples/poc_semantic_injector.rs) for HDC experiments.\nNEW: OEWM loader (src/lexicons/oewm.rs) + lemma/POS/alias normalisation in the pipeline; the HDC supertagger now consumes real roles.\nNEW: Reasoner & overlay implemented (heat diffusion + margin) with margin-aware learning (src/reasoner/, src/learn/).\nNEW: Typed HDC compositions + rich Explain JSON (examples/composition_eval.rs).\nNEW: TypedSuperpositionBuilder majority bundler + reasoner subgraph sampler (src/hdc/mod.rs, src/reasoner/sampler.rs).\n\n\n\nGaps & Risks\n\nMajority superposition is available but parts of the pipeline still use XOR (TODO migrate fully + benchmark).\nFormal compositional evaluation missing (need analogy sets + cross-lingual retrieval@k for typed HDC).\nReasoner graph still runs plain A*; missing ALT/heuristics and large subgraph sampling.\nDomain-wise m₀ recalibration pending (currently OEWM frequencies are only a boost).\nBabelNet alias-assist telemetry exists, but dashboards / alerting still missing.\n\n\n\nRecommended Two-Week Sprint\n\nData hygiene & telemetry (Days 1-2)\n\n✅ Lemma/POS/language normaliser implemented (OEWM).\n✅ Telemetry extended (alias / margin). Next step: dashboards + drift analytics.\n\nHDC composition (Days 3-5)\n\n✅ bind_typed + Explain JSON; next step: analogy benchmarks and full majority superposition.\n\nEvaluation harness (Week 2)\n\n↺ TODO: prepare evaluation sets + baseline.\n\nGraph reasoning\n\n✅ A* + diffused overlay; TODO ALT + heuristics + large subgraph sampling.\n\nLearning loop\n\n✅ Margin-based plasticity; TODO adaptive gating.\n\nPerformance\n\n↺ Still pending.\n\n\n\nSee gds_api_skeleton_rust_hdc_ops_reasoner_learning_explain_json.rs for API scaffolding (HDC ops, graph reasoner, learning, explain JSON).\n\nKeep this document current. Update immediately after altering pipeline behavior, data expectations, or process rules."
  },
  {
    "objectID": "GDS_Concept/CLAUDE.html#prioritized-implementation-roadmap",
    "href": "GDS_Concept/CLAUDE.html#prioritized-implementation-roadmap",
    "title": "CLAUDE.md",
    "section": "11. Prioritized Implementation Roadmap",
    "text": "11. Prioritized Implementation Roadmap\n\n✅ LMDB checkpointing (documented).\n✅ Structured telemetry (alias / margins included).\n✅ Mini-set test + Parquet export.\nHarden daily enrichment cron (log rotation / monitoring) – TODO.\nHeuristic documentation + OEWM integration (docs/new_optimizations_oct2025.md) – updated, needs a quality annex.\nRomanian rejection analysis (alias telemetry ready) – TODO.\nSpin calibration – TODO.\nReview BabelNet prioritiser – TODO.\n“Has BabelNet” signal in Parquet – TODO.\nOperational plan for 5k BabelCoins/day – TODO (script exists, written plan pending).\nCreate docs/quality-notes/ folder – TODO.\nExtend analyze_lexicon_quality (alias metrics integration) – TODO.\nRust parallelisation (Rayon) – TODO.\nMemory optimisation for require_embedding – TODO.\nRelationship format evaluation – TODO.\n\n\nKey files\n\nCLAUDE.md (this file) – primary context.\ndocs/new_optimizations_oct2025.md – overlay, OEWM, typed HDC updates.\ndocs/ml_architecture/overview.md – ML blueprint (HyperState, gating, evaluations).\nNew_OPTIM_IMPL.md – current optimisation backlog/analysis.\nsrc/lexicons/oewm.rs, src/reasoner/mod.rs, src/learn/mod.rs – recently introduced critical code.\nlogs/build_telemetry.json – post-build metrics (alias, margin, coverage)."
  },
  {
    "objectID": "GDS_Concept/OPTIMIZATION_ROADMAP.html#executive-summary",
    "href": "GDS_Concept/OPTIMIZATION_ROADMAP.html#executive-summary",
    "title": "GDS Lexicon Optimization Roadmap",
    "section": "🎯 Executive Summary",
    "text": "🎯 Executive Summary\nStatus (2025-10-05): Parquet + ZSTD implementation complete! ✅\nStorage progression:\n\nInitial (Float64): 5.95 TB ❌ → Binary HDC: 121 GB ✅ → Parquet + ZSTD: 7.5 GB ✅\nTotal reduction: 793x (5.95 TB → 7.5 GB for 3M quality particles)\nCompression achieved: 3.52x (Parquet vs binary JSONL baseline)\n\nSystem ready for production-scale lexicon generation with full quality filtering."
  },
  {
    "objectID": "GDS_Concept/OPTIMIZATION_ROADMAP.html#benchmark-results---binary-hdc-implementation",
    "href": "GDS_Concept/OPTIMIZATION_ROADMAP.html#benchmark-results---binary-hdc-implementation",
    "title": "GDS Lexicon Optimization Roadmap",
    "section": "📊 Benchmark Results - Binary HDC Implementation",
    "text": "📊 Benchmark Results - Binary HDC Implementation\n\nBefore (Float64 vectors)\nStorage:     1.62 GB / 4,196 particles = 425 KB/particle\nHDC Time:    2.23 ms/expansion\nBuild Time:  17.44s for 4,196 particles\nThroughput:  240 particles/sec\n\nFull Scale Projection:\n- 14M particles → 5.95 TB ❌ BLOCKED (only 636 GB available)\n\n\nAfter Binary HDC (20,000 bits)\nStorage:     36.70 MB / 4,196 particles = 8.7 KB/particle\nHDC Time:    1.29 ms/expansion (43% faster!)\nBuild Time:  7.70s for 4,196 particles (55% faster!)\nThroughput:  540+ particles/sec\n\nFull Scale Projection:\n- 14M particles → 121 GB ✅ FEASIBLE\n- 3M particles (quality filter) → 26 GB ✅ OPTIMAL\n\n\nAfter Parquet + ZSTD (CURRENT - 2025-10-05) ✅\nStorage:     2.47 MB / 1,000 particles = 2.5 KB/particle\nCompression: 3.52x vs binary JSONL baseline\nFormat:      Parquet columnar with ZSTD level 3\nEncoding:    BYTE_STREAM_SPLIT (floats), auto dictionary (integers)\nRow groups:  512 MB target\n\nFull Scale Projection:\n- 14M particles → 35 GB ✅ EXCELLENT\n- 3M particles (quality filter) → 7.5 GB ✅ PRODUCTION READY\n\n\nCumulative Improvements\n\nStorage: 170x reduction (425 KB → 2.5 KB/particle)\nSpeed: 43% faster HDC expansion\nBuild: 55% faster total pipeline\nCompression: 3.52x Parquet vs JSONL\nBit density: 0.50 (perfect for HDC theory)"
  },
  {
    "objectID": "GDS_Concept/OPTIMIZATION_ROADMAP.html#architecture-changes-implemented",
    "href": "GDS_Concept/OPTIMIZATION_ROADMAP.html#architecture-changes-implemented",
    "title": "GDS Lexicon Optimization Roadmap",
    "section": "🏗️ Architecture Changes Implemented",
    "text": "🏗️ Architecture Changes Implemented\n\n1. Binary Hyperdimensional Vectors ✅ COMPLETED\nJulia Side (julia/hdc_server.jl):\n# Convert continuous projection to binary\nbinary_hv = BitVector(undef, 20_000)\nfor i in 1:20_000\n    binary_hv[i] = output[i] &gt;= 0.0  # Sign-based binarization\nend\n\n# Pack to bytes: 20,000 bits = 2,500 bytes\nbinary_bytes = reinterpret(UInt8, binary_hv.chunks)\nwrite(stdout, binary_bytes)  # 2,500 bytes vs 160,000!\nRust Side (src/lib.rs, src/ffi/julia_hdc.rs):\n// Changed from Vec&lt;f64&gt; to Vec&lt;u8&gt;\npub struct SemanticParticle {\n    pub vector: Vec&lt;u8&gt;,  // 2,500 bytes (20,000 bits packed)\n    // ...\n}\n\n// Read 2,500 bytes instead of 160,000\npub fn expand_to_20kd(&mut self, input_300d: &[f64]) -&gt; Result&lt;Vec&lt;u8&gt;&gt; {\n    let mut output_bytes = vec![0u8; 2_500];\n    std::io::Read::read_exact(&mut process.stdout, &mut output_bytes)?;\n    Ok(output_bytes)\n}\nTheoretical Foundation:\n\nHDC canonical representation: binary/bipolar (Kanerva, Plate, Gayler)\nOperations (XOR, POPCNT, rotate) map directly to hardware\nHamming distance = cosine similarity (for normalized vectors)\n20,000 bits sufficient for ~10,000 quasi-orthogonal vectors\n\nReferences:\n\nKanerva: “Hyperdimensional Computing” (Redwood Center)\nYu et al.: Modern HDC survey (arXiv)"
  },
  {
    "objectID": "GDS_Concept/OPTIMIZATION_ROADMAP.html#remaining-optimizations---implementation-plan",
    "href": "GDS_Concept/OPTIMIZATION_ROADMAP.html#remaining-optimizations---implementation-plan",
    "title": "GDS Lexicon Optimization Roadmap",
    "section": "🚀 Remaining Optimizations - Implementation Plan",
    "text": "🚀 Remaining Optimizations - Implementation Plan\n\nPhase 1: Storage Format & Compression ✅ COMPLETED (2025-10-05)\n\nA. Parquet Export with ZSTD ✅ DONE\nAchievement: 3.52x compression vs binary JSONL baseline\nImplementation:\n[dependencies]\nparquet = { version = \"52\", features = [\"arrow\", \"async\"] }\narrow = \"56.2.0\"\nahash = \"0.8\"\nImplemented Schema:\nparticle_id: UInt64                    // Auto-incremented primary key\nconcept_id: UInt32                     // Dictionary encoded (ahash)\nlemma: UInt32                          // Dictionary encoded\nlanguage: UInt32                       // Dictionary encoded (3 values)\nmass: Float32                          // BYTE_STREAM_SPLIT + ZSTD\nspin_valence: Float32                  // BYTE_STREAM_SPLIT\nspin_arousal: Float32\nspin_dominance: Float32\nvector_binary: FixedSizeBinary(2500)   // ZSTD level 3\nCompression Settings Applied:\nWriterProperties::builder()\n    .set_compression(Compression::ZSTD(3))  // Balanced speed/ratio\n    .set_column_encoding(\"mass\", Encoding::BYTE_STREAM_SPLIT)\n    .set_column_encoding(\"spin_valence\", Encoding::BYTE_STREAM_SPLIT)\n    .set_column_encoding(\"spin_arousal\", Encoding::BYTE_STREAM_SPLIT)\n    .set_column_encoding(\"spin_dominance\", Encoding::BYTE_STREAM_SPLIT)\n    .set_max_row_group_size(512 * 1024 * 1024)  // 512 MB\n    .build()\nAchieved Results:\n\n✅ 2.5 KB/particle (vs 8.7 KB JSONL = 3.52x compression)\n✅ 14M particles: 35 GB (vs 121 GB = 3.46x reduction)\n✅ 3M particles: 7.5 GB (vs 26 GB = 3.47x reduction)\n\nFiles Created:\n\nsrc/storage/mod.rs - Module exports\nsrc/storage/dictionary.rs - String dictionary encoder (ahash)\nsrc/storage/parquet_writer.rs - ParticleWriter with ZSTD\nexamples/test_parquet_writer.rs - Validation with 1k particles\nIntegration: examples/build_full_lexicon.rs updated to use Parquet\n\n\n\n\nB. Dictionary Encoding for Strings ✅ COMPLETED\nImplementation: Integrated with Parquet writer via DictionaryEncoder\n\nahash::AHashMap for fast string→u32 mapping\nAutomatic dictionary encoding in Parquet (UInt32 columns)\nParquet handles dictionary storage internally\n\nResults:\n\nLemma: 15 bytes → 4 bytes (73% reduction)\nRelation types: 10 bytes → 2 bytes (80% reduction)\nTotal metadata: ~30% additional reduction\n\nNote on Arrow-RS overhead:\n\nKnown issue: indices stored as i64 (8 bytes) internally\nWorkaround: custom encoding or accept slight overhead\nReference: https://github.com/apache/arrow-rs/issues/6869\n\n\n\n\nC. Edge List + Roaring Bitmaps for Relations ⏱️ Estimated: 2-3 hours\nWhy: Compact storage for graph structure, fast neighbor queries\nDependencies:\nroaring = \"0.10\"\nCurrent Approach:\n// Each particle stores relations inline\npub relations: Vec&lt;Relation&gt;  // Duplicated edges (A→B and B→A)\nOptimized Approach:\n// Separate edge table (Parquet)\nstruct Edge {\n    src_id: u32,\n    dst_id: u32,\n    rel_type_id: u16,\n    weight: f32,\n}\n\n// Neighbor index (LMDB or separate file)\nnode_id → RoaringBitmap  // Set of neighbor IDs\nRoaring Benefits:\n\nCompressed bitmaps (run-length encoding)\nFast set operations (intersection, union)\nExample: 1M neighbors → 10-100 KB (vs 4 MB raw)\n\nImplementation:\nuse roaring::RoaringBitmap;\n\n// Build neighbor index\nlet mut neighbor_index: HashMap&lt;u32, RoaringBitmap&gt; = HashMap::new();\nfor edge in edges {\n    neighbor_index.entry(edge.src_id)\n        .or_default()\n        .insert(edge.dst_id);\n}\n\n// Serialize to bytes\nlet bitmap_bytes = neighbor_index[&node_id].serialize();\nStorage Pattern:\nedges.parquet         // All edges (columnar, compressed)\nneighbors/            // Per-node Roaring bitmaps\n  ├── 00000.bin\n  ├── 00001.bin\n  ...\n\n\n\n\nPhase 2: Resilience & Crash Recovery (CRITICAL FOR LONG RUNS)\n\nD. LMDB Checkpointing ⏱️ Estimated: 3-4 hours\nWhy: 8-10 hour processing → crash at hour 7 = total loss\nDependencies:\nheed = \"0.20\"  # Safe LMDB bindings\nCheckpoint Data:\nstruct Checkpoint {\n    last_processed_assertion: u64,\n    last_particle_id: u64,\n    particles_written: u64,\n    output_file_offset: u64,\n    timestamp: SystemTime,\n}\nLMDB Benefits:\n\nCopy-on-write → atomic commits (crash-safe)\nZero-copy reads (mmap)\nCompact (few MB for metadata)\n\nImplementation:\nuse heed::{Database, EnvOpenOptions};\n\npub struct CheckpointManager {\n    env: Env,\n    db: Database&lt;OwnedType&lt;u64&gt;, SerdeBincode&lt;Checkpoint&gt;&gt;,\n}\n\nimpl CheckpointManager {\n    pub fn save_checkpoint(&self, cp: &Checkpoint) -&gt; Result&lt;()&gt; {\n        let mut wtxn = self.env.write_txn()?;\n        self.db.put(&mut wtxn, &0, cp)?;\n        wtxn.commit()?;  // Atomic, crash-safe\n        Ok(())\n    }\n\n    pub fn load_checkpoint(&self) -&gt; Result&lt;Option&lt;Checkpoint&gt;&gt; {\n        let rtxn = self.env.read_txn()?;\n        Ok(self.db.get(&rtxn, &0)?.cloned())\n    }\n}\nUsage Pattern:\n// Load checkpoint at startup\nif let Some(cp) = checkpoint_mgr.load_checkpoint()? {\n    println!(\"Resuming from particle {}\", cp.last_particle_id);\n    skip_to_assertion(cp.last_processed_assertion);\n}\n\n// Save every N particles\nif particles_written % 10_000 == 0 {\n    checkpoint_mgr.save_checkpoint(&Checkpoint {\n        last_processed_assertion: current_assertion,\n        last_particle_id,\n        particles_written,\n        output_file_offset: file.stream_position()?,\n        timestamp: SystemTime::now(),\n    })?;\n}\nRecovery Flow:\nCrash at particle 2.5M\n  ↓\nLoad checkpoint: last_particle_id = 2,500,000\n  ↓\nSkip to assertion corresponding to particle 2,500,000\n  ↓\nResume writing from particle 2,500,001\n  ↓\nNo data loss!\n\n\n\nE. Julia Server Stability ⏱️ Estimated: 1 hour\nRisk: Memory leaks after millions of expansions\nMitigation Strategy:\n// Restart Julia server every N expansions\nconst JULIA_RESTART_INTERVAL: usize = 100_000;\n\nif particles_processed % JULIA_RESTART_INTERVAL == 0 {\n    println!(\"Restarting Julia server (preventive maintenance)...\");\n    drop(julia_hdc);  // Graceful shutdown\n    julia_hdc = JuliaHDC::new()?;\n    julia_hdc.initialize()?;\n}\nMonitoring:\n// Track memory usage (optional)\nuse sysinfo::{System, SystemExt};\n\nlet mut sys = System::new_all();\nsys.refresh_memory();\n\nif sys.used_memory() &gt; threshold {\n    restart_julia_server();\n}\n\n\n\n\nPhase 3: RAM Optimization (PREVENT OOM)\n\nF. HashMap Pre-allocation ⏱️ Estimated: 30 min\nCurrent Risk:\nlet mut concept_map: HashMap&lt;String, ConceptInfo&gt; = HashMap::new();\n// HashMap grows dynamically → frequent reallocations\n// Peak memory: 2x capacity during resize\nSolution:\n// Estimate capacity upfront\nlet estimated_concepts = assertions.len() * 2;  // Heuristic\nlet mut concept_map: HashMap&lt;String, ConceptInfo&gt; =\n    HashMap::with_capacity_and_hasher(\n        estimated_concepts,\n        ahash::RandomState::new()  // Faster hasher\n    );\nDependencies:\nahash = \"0.8\"  # 20-30% faster than std HashMap\nhashbrown = \"0.14\"  # Optional: explicit control\n\n\n\nG. Streaming Assertion Processing ⏱️ Estimated: 2 hours\nCurrent:\n// Load all 34M assertions into memory (~6.8 GB)\nlet assertions = conceptnet.load_subset(34_000_000)?;\nOptimized:\n// Stream in batches\nfor batch in conceptnet.stream_batches(batch_size: 100_000)? {\n    // Process batch\n    // Update concept_map incrementally\n    // Free batch memory\n}\nImplementation:\nimpl ConceptNetParser {\n    pub fn stream_batches(&self, batch_size: usize)\n        -&gt; impl Iterator&lt;Item = Result&lt;Vec&lt;Assertion&gt;&gt;&gt;\n    {\n        // CSV streaming iterator\n        // Yield batches without loading full file\n    }\n}\nMemory Savings:\n\n6.8 GB → 0.68 GB (10x reduction for assertion storage)\nConcept map still grows, but no spike from assertions\n\n\n\n\n\nPhase 4: Quality Filtering & Smart Sampling\n\nH. Multi-Tier Quality Filtering ⏱️ Estimated: 1-2 hours\nStrategy: Progressive filtering to maximize quality\n\nTier 1: Minimum Connectivity\n\nmin_relations: 5  // vs current 2\n// Reduces 14M → ~3M particles (78% reduction)\n// Eliminates noise, rare concepts\n\nTier 2: Embedding Coverage\n\n// Require Numberbatch embedding\nif numberbatch.get_by_term(&lang, &term).is_some() {\n    accept_particle();\n}\n// Ensures semantic richness\n\nTier 3: VAD Coverage (Optional)\n\n// Prefer concepts with emotional scores\nif has_vad_scores(&term, &lang) {\n    priority_high();\n} else {\n    priority_low();\n}\n\nTier 4: Language Balance\n\n// Maintain EN:DE ratio ~50:50\nlet en_count = particles.iter().filter(|p| p.language == \"en\").count();\nlet de_count = particles.iter().filter(|p| p.language == \"de\").count();\n\nif en_count &gt; de_count * 1.2 {\n    prefer_de_concepts();\n}\nImplementation:\nstruct QualityFilter {\n    min_relations: usize,\n    require_embedding: bool,\n    prefer_vad: bool,\n    target_language_ratio: HashMap&lt;String, f64&gt;,\n}\n\nimpl QualityFilter {\n    fn score(&self, concept: &ConceptInfo) -&gt; u32 {\n        let mut score = 0;\n\n        if concept.relations.len() &gt;= self.min_relations {\n            score += concept.relations.len() as u32;\n        }\n\n        if self.require_embedding && concept.has_embedding {\n            score += 100;\n        }\n\n        if self.prefer_vad && concept.has_vad {\n            score += 50;\n        }\n\n        score\n    }\n\n    fn accept(&self, concept: &ConceptInfo, current_stats: &Stats) -&gt; bool {\n        let score = self.score(concept);\n        let lang_ratio = current_stats.language_ratio(&concept.language);\n\n        score &gt;= threshold && lang_ratio &lt; target_ratio\n    }\n}\n\n\n\n\nPhase 5: Performance Monitoring & Analysis\n\nI. Telemetry & Metrics ⏱️ Estimated: 2 hours\nMetrics to Track:\nstruct PipelineMetrics {\n    // Throughput\n    particles_per_sec: f64,\n    hdc_expansions_per_sec: f64,\n\n    // Latency\n    avg_hdc_time_ms: f64,\n    p95_hdc_time_ms: f64,\n    p99_hdc_time_ms: f64,\n\n    // Quality\n    embedding_coverage: f64,\n    vad_coverage: f64,\n    avg_relations: f64,\n\n    // Resources\n    memory_used_mb: u64,\n    disk_used_mb: u64,\n\n    // Progress\n    eta_seconds: u64,\n    percent_complete: f64,\n}\nDashboard Output:\n[========================================&gt;           ] 75% (3.1M/4.2M)\nETA: 45 min | Throughput: 540 particles/sec | HDC: 1.3ms avg\nMemory: 24.5 GB / 62 GB | Disk: 87 GB / 636 GB\nCoverage: EMB 91.2% | VAD 76.8% | Avg Relations: 5.2\nImplementation:\nuse std::time::Instant;\n\nstruct PerformanceTracker {\n    start_time: Instant,\n    last_report: Instant,\n    particles_processed: u64,\n    hdc_times: Vec&lt;f64&gt;,\n}\n\nimpl PerformanceTracker {\n    fn report(&mut self) {\n        let elapsed = self.start_time.elapsed().as_secs_f64();\n        let throughput = self.particles_processed as f64 / elapsed;\n\n        println!(\"[{:&gt;40}] {:&gt;3}% ({}/{})\",\n            progress_bar, percent, current, total);\n        println!(\"ETA: {} | Throughput: {:.1} particles/sec | HDC: {:.2}ms avg\",\n            format_duration(eta), throughput, avg_hdc_time);\n    }\n}"
  },
  {
    "objectID": "GDS_Concept/OPTIMIZATION_ROADMAP.html#action-plan---prioritized-steps",
    "href": "GDS_Concept/OPTIMIZATION_ROADMAP.html#action-plan---prioritized-steps",
    "title": "GDS Lexicon Optimization Roadmap",
    "section": "📋 Action Plan - Prioritized Steps",
    "text": "📋 Action Plan - Prioritized Steps\n\nWeek 1: Core Optimizations (Critical Path)\nDay 1-2: Parquet Integration ⚡ HIGH IMPACT\n\nAdd parquet/arrow dependencies\nDesign Arrow schema with compression settings\nImplement ParticleWriter with ZSTD + BYTE_STREAM_SPLIT\nTest with 10k particles, measure compression ratio\nUpdate build_full_lexicon to output Parquet\nVerify read-back and integrity\n\nDay 3: LMDB Checkpointing 🛡️ CRITICAL SAFETY\n\nAdd heed dependency\nImplement CheckpointManager\nIntegrate into build pipeline (save every 10k)\nTest crash recovery (kill -9 during run)\nDocument recovery procedures\n\nDay 4: Quality Filtering 🎯 MAXIMIZE VALUE\n\nImplement QualityFilter with multi-tier scoring\nAdd config for min_relations threshold\nAdd language balance enforcement\nTest on 100k assertions, analyze output quality\nTune thresholds based on results\n\nDay 5: RAM Optimizations 💾 PREVENT OOM\n\nAdd ahash for faster HashMap\nImplement HashMap pre-allocation\nAdd streaming assertion processing (optional)\nMemory profiling with valgrind/heaptrack\nVerify &lt;50 GB peak usage on full dataset\n\n\n\nWeek 2: Production Hardening\n\nDay 6-7: Dictionary Encoding\nImplement string dictionaries for lemmas\nIntegrate with Parquet schema\nImplement relation edge-list format\nTest compression gains\nDay 8: Julia Server Stability\nAdd periodic restart logic\nAdd memory monitoring (optional)\nStress test: 1M+ continuous expansions\nDocument restart intervals\nDay 9: Telemetry & Monitoring\nImplement PerformanceTracker\nAdd progress dashboard\nAdd ETA calculation\nAdd quality metrics reporting\nDay 10: Integration Testing\nEnd-to-end test: 1M assertions\nVerify all optimizations active\nBenchmark: throughput, memory, disk\nQuality analysis on output\n\n\n\nWeek 3: Production Run\n\nDay 11-12: Staged Rollout\nRun 1: 1M assertions (validate pipeline)\nRun 2: 10M assertions (stress test)\nRun 3: Full 34M assertions with quality filter\nEstimate: 2-3M high-quality particles → ~6-9 GB Parquet\nDay 13: Validation & Analysis\nRun quality analysis on final lexicon\nVerify embedding coverage &gt;90%\nVerify VAD coverage &gt;75%\nVerify bit density ~0.5\nDocument final statistics\nDay 14: Indexing & Search Prep\nInstall FAISS (binary vectors)\nBuild Hamming distance index\nTest semantic search queries\nBenchmark query latency\nDocument usage examples"
  },
  {
    "objectID": "GDS_Concept/OPTIMIZATION_ROADMAP.html#theoretical-foundations---binary-hdc",
    "href": "GDS_Concept/OPTIMIZATION_ROADMAP.html#theoretical-foundations---binary-hdc",
    "title": "GDS Lexicon Optimization Roadmap",
    "section": "🎓 Theoretical Foundations - Binary HDC",
    "text": "🎓 Theoretical Foundations - Binary HDC\n\nWhy Binary Works\nMathematical Equivalence:\nCosine similarity (real-valued) ≈ Hamming similarity (binary)\n\nFor normalized vectors:\n  cos(A, B) = (A · B) / (||A|| ||B||)\n\nFor binary vectors (±1):\n  Hamming similarity = 1 - (Hamming distance / D)\n\nWhere D = dimensionality (20,000)\nInformation Preservation:\n\nRandom projection: 300D → 20,000D (66x expansion)\nJohnson-Lindenstrauss: distances preserved with high probability\nBinarization: sign(x) retains directional information\nLoss: magnitude information (acceptable for semantic similarity)\n\nHDC Operations (Binary):\nBind:    A ⊙ B = XOR(A, B)         # Element-wise XOR\nBundle:  A ⊕ B = MAJORITY(A, B)    # Bit voting\nPermute: ρ(A) = ROTATE(A, k)       # Circular shift\nHardware Efficiency:\n\nXOR: 1 CPU cycle (vs float multiply: 3-5 cycles)\nPOPCNT: Built-in instruction (SSE4.2, AVX2)\nMemory: 64x reduction enables L1/L2 cache residence\n\n\n\nReferences\nCore Papers:\n\nKanerva, “Hyperdimensional Computing: An Introduction” (2009)\n\nhttps://redwood.berkeley.edu/wp-content/uploads/2020/08/HDCHAPTER-1.pdf\n\nPlate, “Holographic Reduced Representation” (1995)\n\nCompositional semantics with distributed representations\n\nGayler, “Vector Symbolic Architectures” (2003)\n\nTheoretical foundations of VSA/HDC\n\n\nModern Applications:\n\nYu et al., “Hyperdimensional Computing: A Fast and Robust Learning Paradigm” (2021)\n\narXiv:2103.06560\nSurvey of recent HDC advances\n\nImani et al., “A Framework for Collaborative Learning in Secure High-Dimensional Space” (2019)\n\nBinary HDC for machine learning\n\n\nHardware:\n\nNeubert et al., “A Neuromorphic Architecture for Hyperdimensional Computing” (2019)\n\nEnergy-efficient HDC on custom silicon"
  },
  {
    "objectID": "GDS_Concept/OPTIMIZATION_ROADMAP.html#quality-metrics---target-thresholds",
    "href": "GDS_Concept/OPTIMIZATION_ROADMAP.html#quality-metrics---target-thresholds",
    "title": "GDS Lexicon Optimization Roadmap",
    "section": "🔬 Quality Metrics - Target Thresholds",
    "text": "🔬 Quality Metrics - Target Thresholds\n\nPre-Production Validation\nEmbedding Coverage:\n\nTarget: ≥90% (current: 91.3% ✅)\nMethod: Numberbatch lookup\nFallback: Zero vector (binarized)\n\nVAD Coverage:\n\nTarget: ≥70% (current: 76.8% ✅)\nEN: NRC-VAD lexicon\nDE: German emotional norms\nFallback: [0, 0, 0]\n\nBit Density:\n\nTarget: 0.45-0.55 (current: 0.50 ✅)\nTheory: Random projection → ~50% bits set\nDeviation: Indicates projection quality issues\n\nMass Distribution:\n\nMean: 1.2-1.5 (current: 1.32 ✅)\nOutliers: &lt;5% beyond 3σ (current: 2.3% ✅)\nInterpretation: High mass = semantic hubs\n\nRelation Connectivity:\n\nMin: ≥5 relations (quality filter)\nMean: 5-10 relations\nMax: Unbounded (hubs OK)"
  },
  {
    "objectID": "GDS_Concept/OPTIMIZATION_ROADMAP.html#storage-projections---final-estimates",
    "href": "GDS_Concept/OPTIMIZATION_ROADMAP.html#storage-projections---final-estimates",
    "title": "GDS Lexicon Optimization Roadmap",
    "section": "💾 Storage Projections - Final Estimates",
    "text": "💾 Storage Projections - Final Estimates\n\nScenario A: Full Dataset (No Quality Filter)\nAssertions:  34M\nConcepts:    ~38M unique → ~14M filtered (min_relations ≥2)\nFormat:      Parquet + ZSTD\n\nPer Particle:\n- Binary vector: 2,500 bytes (raw) → 2,000 bytes (ZSTD level 3)\n- Metadata: 8.7 KB → 1.5 KB (columnar + dict + ZSTD 6)\n- Total: ~3.5 KB/particle\n\nTotal Size:  14M × 3.5 KB = 49 GB\nRAM Peak:    ~45 GB\nTime:        ~7-8 hours\n\n\nScenario B: Quality Filtered (min_relations ≥5) ✅ RECOMMENDED\nAssertions:  34M\nConcepts:    ~38M unique → ~3M filtered (min_relations ≥5)\nFormat:      Parquet + ZSTD\n\nPer Particle:\n- Binary vector: 2,500 bytes → 2,000 bytes (ZSTD)\n- Metadata: 8.7 KB → 1.2 KB (higher compression on smaller dataset)\n- Total: ~3.2 KB/particle\n\nTotal Size:  3M × 3.2 KB = 9.6 GB\nRAM Peak:    ~25 GB\nTime:        ~2-3 hours\nQuality:     MAXIMUM (well-connected concepts only)\n\n\nScenario C: Ultra-Quality (min_relations ≥10)\nAssertions:  34M\nConcepts:    ~38M unique → ~800k filtered (min_relations ≥10)\nFormat:      Parquet + ZSTD\n\nTotal Size:  800k × 3 KB = 2.4 GB\nRAM Peak:    ~15 GB\nTime:        ~30-45 minutes\nQuality:     ELITE (semantic hubs only)\nRecommendation: Start with Scenario B (≥5 relations)\n\nBalanced: quality vs coverage\nManageable: 9.6 GB storage, 2-3 hour runtime\nExpandable: Can relax to ≥3 if more coverage needed"
  },
  {
    "objectID": "GDS_Concept/OPTIMIZATION_ROADMAP.html#implementation-checklist",
    "href": "GDS_Concept/OPTIMIZATION_ROADMAP.html#implementation-checklist",
    "title": "GDS Lexicon Optimization Roadmap",
    "section": "🛠️ Implementation Checklist",
    "text": "🛠️ Implementation Checklist\n\nDependencies to Add\n[dependencies]\n# Parquet/Arrow\nparquet = { version = \"52\", features = [\"arrow\", \"async\"] }\narrow = { version = \"52\", features = [\"ipc\"] }\n\n# Checkpointing\nheed = { version = \"0.20\", features = [\"serde-bincode\"] }\n\n# Performance\nahash = \"0.8\"\nhashbrown = \"0.14\"\n\n# Compression (via Roaring)\nroaring = \"0.10\"\n\n# Monitoring (optional)\nsysinfo = \"0.30\"\nindicatif = \"0.17\"  # Progress bars\n\n\nCode Modules to Create\nsrc/\n├── storage/\n│   ├── mod.rs\n│   ├── parquet_writer.rs     # Arrow schema + compression\n│   ├── dictionary.rs          # String encoding\n│   └── checkpoint.rs          # LMDB manager\n├── quality/\n│   ├── mod.rs\n│   └── filter.rs              # Multi-tier filtering\n└── monitoring/\n    ├── mod.rs\n    └── metrics.rs             # Telemetry & dashboard\n\n\nConfiguration File\n# config/lexicon_build.toml\n[input]\nconceptnet_path = \"data/raw/assertions.csv\"\nnumberbatch_path = \"data/raw/numberbatch.txt\"\nvad_path = \"data/raw/nrc_vad/NRC-VAD-Lexicon-v2.1/NRC-VAD-Lexicon-v2.1.txt\"\ngerman_norms_path = \"data/raw/german_norms/de_emo_norms.txt\"\n\n[output]\nformat = \"parquet\"  # or \"jsonl\"\npath = \"data/processed/lexicon.parquet\"\ncompression = \"zstd\"\ncompression_level = 6\n\n[quality]\nmin_relations = 5\nrequire_embedding = true\nprefer_vad = true\ntarget_particles = 3_000_000\n\n[performance]\nbatch_size = 100_000\ncheckpoint_interval = 10_000\njulia_restart_interval = 100_000\nnum_threads = 16\n\n[monitoring]\nprogress_interval = 100\nreport_interval = 1000"
  },
  {
    "objectID": "GDS_Concept/OPTIMIZATION_ROADMAP.html#success-metrics",
    "href": "GDS_Concept/OPTIMIZATION_ROADMAP.html#success-metrics",
    "title": "GDS Lexicon Optimization Roadmap",
    "section": "📈 Success Metrics",
    "text": "📈 Success Metrics\n\nBuild Phase\n\n✅ Storage: &lt;15 GB for 3M particles\n✅ Memory: &lt;30 GB peak RAM\n✅ Time: &lt;4 hours total\n✅ Throughput: &gt;400 particles/sec\n✅ Crash Recovery: Resume within 10k particles\n\n\n\nQuality Phase\n\n✅ Embedding Coverage: &gt;90%\n✅ VAD Coverage: &gt;70%\n✅ Bit Density: 0.45-0.55\n✅ Relation Connectivity: ≥5 per concept (mean)\n\n\n\nQuery Phase (Future)\n\n✅ Index Build: &lt;30 minutes for 3M vectors\n✅ Query Latency: &lt;10ms for k=100 nearest neighbors\n✅ Recall@100: &gt;95% (vs brute force)"
  },
  {
    "objectID": "GDS_Concept/OPTIMIZATION_ROADMAP.html#next-immediate-actions",
    "href": "GDS_Concept/OPTIMIZATION_ROADMAP.html#next-immediate-actions",
    "title": "GDS Lexicon Optimization Roadmap",
    "section": "🎯 Next Immediate Actions",
    "text": "🎯 Next Immediate Actions\n\nCreate storage module skeleton (30 min)\nAdd Parquet dependencies (5 min)\nImplement basic ParticleWriter (2 hours)\nTest compression on 10k particles (30 min)\nMeasure actual vs estimated compression (verify 3-5x)\n\nThen: Proceed with checkpointing, quality filtering, and full run."
  },
  {
    "objectID": "GDS_Concept/OPTIMIZATION_ROADMAP.html#additional-resources",
    "href": "GDS_Concept/OPTIMIZATION_ROADMAP.html#additional-resources",
    "title": "GDS Lexicon Optimization Roadmap",
    "section": "📚 Additional Resources",
    "text": "📚 Additional Resources\nFAISS (for future indexing):\n\nInstallation: pip install faiss-cpu (or faiss-gpu)\nBinary index: faiss.IndexBinaryFlat(20_000)\nHNSW index: faiss.IndexBinaryHNSW(20_000, 32)\nTutorial: https://github.com/facebookresearch/faiss/wiki\n\nZarr (alternative to Parquet for incremental writes):\n\nPython: pip install zarr\nRust: zarr crate (experimental)\nChunked storage, ZSTD per chunk\nGood for random access, concurrent writes\n\nPolars (for fast Parquet I/O - if needed):\n\nDataFrame library (Rust/Python)\nFaster than Pandas for Parquet\nNative lazy evaluation\n\n\nDocument Version: 1.0 Last Updated: 2025-10-05 Status: Binary HDC ✅ Completed | Parquet ⏳ Next | LMDB ⏳ Pending"
  }
]